{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad3c776",
   "metadata": {},
   "source": [
    "# 1. Target Definition\n",
    "\n",
    "#### Purpose\n",
    "\n",
    "This analysis develops predictive models to estimate two company-level operational greenhouse gas emission targets consistent with the Greenhouse Gas Protocol. Accurate, auditable estimates enable downstream ESG reporting, portfolio-level exposure analysis, and risk assessment when disclosures are incomplete.\n",
    "\n",
    "#### Definitions\n",
    "\n",
    "- **Scope 1 — Direct emissions:** Emissions produced by sources owned or directly controlled by the reporting entity (e.g., combustion on site, company vehicle fleets, fugitive process emissions).\n",
    "- **Scope 2 — Indirect emissions (purchased energy):** Emissions attributable to the generation of purchased electricity, heat or steam consumed by the company.\n",
    "\n",
    "#### Why both targets are modelled separately\n",
    "\n",
    "- Scope 1 reflects operational intensity and asset ownership; drivers include production scale, fuel use and sector technology.\n",
    "- Scope 2 depends heavily on electricity consumption patterns and grid carbon intensity, which vary by geography and operations.\n",
    "\n",
    "Practical implication: Models should be trained separately for each target while sharing thoughtfully engineered features where appropriate (e.g., revenue, sector exposure, adjusted ESG signals)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8bdd1f",
   "metadata": {},
   "source": [
    "# 2. Use Cases and Practical Importance\n",
    "\n",
    "#### Context and stakeholders:\n",
    "\n",
    "- Financial institutions and asset managers require emission estimates for portfolio-level carbon accounting, scenario analysis, and regulatory reporting.\n",
    "- Corporates and supply‑chain managers use estimates to prioritize engagement and mitigation planning where direct measurements are missing.\n",
    "- Rating agencies and research teams use modelled emissions to complement disclosures and fill data gaps.\n",
    "\n",
    "#### Operational priorities for model design:\n",
    "\n",
    "- Stability and interpretability of central estimates are often more valuable than exact fits for extreme industrial outliers.\n",
    "- Predictions should be auditable and reproducible; pre-processing and clipping rules must be documented and conservative.\n",
    "\n",
    "#### Decision guidance:\n",
    "\n",
    "- Prioritize models and metrics that produce low median error across the portfolio.\n",
    "- Provide uncertainty-aware outputs (e.g., prediction intervals) where feasible to flag high-risk estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b02403",
   "metadata": {},
   "source": [
    "# 3. Target Distribution and ML Objectives\n",
    "\n",
    "## 3.1 Observed distribution\n",
    "\n",
    "- Both Scope 1 and Scope 2 show pronounced right skew and heavy tails. A handful of large industrial firms contribute a disproportionately large share of total emissions.\n",
    "- Emissions variability is often multiplicative with company scale (revenue), making additive residual models less appropriate without transformation.\n",
    "- Applying a log1p transform stabilizes variance and reduces skewness, producing distributions that are more amenable to standard supervised learners.\n",
    "\n",
    "#### Implications for modelling:\n",
    "\n",
    "- Prefer log-transformed diagnostics for residual analysis and consider log-based metrics for stability.\n",
    "- Maintain separate models for Scope 1 and Scope 2 but use shared pre-processing conventions where appropriate (e.g., clipping and documented transforms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae4d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('data/train.csv')\n",
    "\n",
    "# Plot distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Scope 1\n",
    "sns.histplot(df['target_scope_1'], kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Target Scope 1 Distribution')\n",
    "\n",
    "# Scope 2\n",
    "sns.histplot(df['target_scope_2'], kde=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Target Scope 2 Distribution')\n",
    "\n",
    "# Log Scope 1\n",
    "sns.histplot(np.log1p(df['target_scope_1']), kde=True, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Log(Target Scope 1) Distribution')\n",
    "\n",
    "# Log Scope 2\n",
    "sns.histplot(np.log1p(df['target_scope_2']), kde=True, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Log(Target Scope 2) Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e7c8db",
   "metadata": {},
   "source": [
    "## 3.2 Why Median-Focused Prediction\n",
    "\n",
    "#### Rationale:\n",
    "\n",
    "- Extreme emission values are frequently driven by asset-level characteristics (e.g., power plants, refineries) that are not observable in the provided dataset; attempting to fit those outliers closely will likely overfit.\n",
    "- Stakeholders typically use aggregated or median estimates at portfolio or sector levels — therefore stability of central estimates is more valuable than perfect tail accuracy.\n",
    "- RMSE is sensitive to a small number of extreme errors; MAE and log-MAE are more robust and align with multiplicative emission behavior.\n",
    "\n",
    "#### Operational guidance:\n",
    "\n",
    "- Use MAE as the primary selection metric and log-MAE/log-RMSE as complementary diagnostics.\n",
    "- Consider percentile clipping or targeted capping of extreme values during pre-processing to limit their influence while documenting changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbec3bc6",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering — Overview\n",
    "\n",
    "This section summarizes feature families, the rationale behind each group, and recommended transformations. The goal is to produce robust, interpretable predictors for Scope 1 and Scope 2 while avoiding implicit data leakage.\n",
    "\n",
    "#### Feature families\n",
    "\n",
    "1. Scale (Operational Size)\n",
    "- Rationale: Revenue is a proxy for production scale and energy use.\n",
    "- Candidate features: `revenue`, `log_revenue`, sector-partitioned absolute revenues, NACE revenue shares.\n",
    "- Recommendation: Provide both raw and log-transformed variants; derive scope-specific absolute revenues.\n",
    "\n",
    "2. Geography and Energy Context\n",
    "- Rationale: Grid carbon intensity and national energy mixes drive Scope 2 variation; local practices affect Scope 1 modestly.\n",
    "- Candidate features: `country_code` (categorical), region groups (collapse rare categories), country emission-factor lookups where available.\n",
    "- Recommendation: Use stable encodings (group rare categories, regularized target encoding for high-cardinality country codes).\n",
    "\n",
    "3. Behavioural & Sustainability Signals\n",
    "- Rationale: ESG scores, environmental activity adjustments and SDG commitments proxy for mitigation efforts and process maturity.\n",
    "- Candidate features: adjusted `environmental_score`, `social_score`, `governance_score`, aggregated `env_score_adjustment`, SDG presence indicators.\n",
    "- Recommendation: Use adjusted environmental score rather than composite overall score to avoid multicollinearity; clip and standardize consistently across splits.\n",
    "\n",
    "#### General principles\n",
    "- Document all transformations and clipping rules; avoid imputations that could introduce bias unless explicitly justified.\n",
    "- Engineer target-specific features (e.g., `log_scope_1_revenue`) to better align predictors with each emission target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dfb69f",
   "metadata": {},
   "source": [
    "# 5. Scale of Business\n",
    "\n",
    "This section examines the hypothesis that company scale (revenue) is a core driver of emissions and presents evidence and recommendations.\n",
    "\n",
    "## 5.1 Hypothesis 1\n",
    "\n",
    "> Business scale (as proxied by revenue) is the dominant single predictor of operational emissions.\n",
    "\n",
    "#### Evidence\n",
    "\n",
    "- Correlation analyses show a substantial monotonic relationship between `log(revenue)` and `log(total emissions)`.\n",
    "- Visual inspection (histograms and log-transformed histograms) confirms multiplicative behavior across scales.\n",
    "\n",
    "#### Interpretation and practical takeaways\n",
    "\n",
    "- Include both `revenue` and `log_revenue` as baseline features; `log_revenue` stabilizes variance for linear models.\n",
    "- Scale alone is informative but incomplete — combine scale features with sector and geography for improved specificity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c5c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis 1: Revenue vs Emissions\n",
    "\n",
    "# 1. Plot Revenue Distribution (Raw vs Log)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.histplot(df['revenue'], kde=True, ax=axes[0])\n",
    "axes[0].set_title('Revenue Distribution (Raw)')\n",
    "\n",
    "sns.histplot(np.log1p(df['revenue']), kde=True, ax=axes[1])\n",
    "axes[1].set_title('Revenue Distribution (Log)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Calculate Log Correlations\n",
    "df['log_revenue'] = np.log1p(df['revenue'])\n",
    "df['log_total_emissions'] = np.log1p(df['target_scope_1'] + df['target_scope_2'])\n",
    "\n",
    "correlation = df['log_revenue'].corr(df['log_total_emissions'])\n",
    "print(f\"Correlation between Log(Revenue) and Log(Total Emissions): {correlation:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcadab4",
   "metadata": {},
   "source": [
    "## 5.2 Hypothesis 2\n",
    "\n",
    "> The sectoral mix of a company’s revenue modifies the relationship between revenue and each emission target; sector-partitioned revenues therefore provide target-specific predictive power.\n",
    "\n",
    "#### Procedure summary\n",
    "\n",
    "1. Map NACE sectors to scope exposure categories (scope‑1, scope‑2, both, or neither) using conservative domain rules by consulting with a domain expert (=llm)\n",
    "2. Multiply company revenue by sector revenue_pct to compute absolute revenue per sector.\n",
    "3. Aggregate sector revenues into `scope_1_revenue` and `scope_2_revenue` buckets.\n",
    "4. Create features: `log_scope_1_revenue`, `log_scope_2_revenue`, and binary materiality flags.\n",
    "\n",
    "#### Evidence and rationale\n",
    "\n",
    "- Sectors with direct process emissions (manufacturing, mining, heavy industry) are stronger predictors for Scope 1.\n",
    "- Service or ICT sectors typically correlate more with electricity consumption and hence Scope 2.\n",
    "- Empirical correlations within non-zero subsets support the increased predictive value of scope-partitioned revenues.\n",
    "\n",
    "#### Recommended features\n",
    "\n",
    "- `log_scope_1_revenue`, `log_scope_2_revenue` (primary engineered predictors).\n",
    "- Binary presence flags indicating material sector exposure (helps tree-based learners capture sparse signals)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa20bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis 2: Sector Analysis\n",
    "\n",
    "# Load sector data and classification\n",
    "sector_df = pd.read_csv('data/revenue_distribution_by_sector.csv')\n",
    "classification_df = pd.read_csv('data/sector_emission_scope_classification.csv')\n",
    "\n",
    "# Merge classification\n",
    "sector_df = pd.merge(sector_df, classification_df, on='nace_level_2_name', how='left')\n",
    "\n",
    "# Fill missing classifications (if any) with False/True default (Scope 2 is universal)\n",
    "sector_df['affects_scope_1'] = sector_df['affects_scope_1'].fillna(False)\n",
    "sector_df['affects_scope_2'] = sector_df['affects_scope_2'].fillna(True)\n",
    "\n",
    "# Create summary table\n",
    "sector_summary = sector_df.groupby(['nace_level_1_code', 'nace_level_1_name', 'nace_level_2_name'])[['affects_scope_1', 'affects_scope_2']].first().reset_index()\n",
    "sector_summary['company_count'] = sector_df.groupby(['nace_level_1_code', 'nace_level_1_name', 'nace_level_2_name']).size().values\n",
    "\n",
    "# Display table\n",
    "from IPython.display import display\n",
    "display(sector_summary)\n",
    "\n",
    "# Calculate Scope 1 / Scope 2 Revenue\n",
    "merged_df = pd.merge(sector_df, df[['entity_id', 'revenue']], on='entity_id', how='inner')\n",
    "\n",
    "# Calculate weighted revenue for each scope\n",
    "# Note: A sector can contribute to BOTH Scope 1 and Scope 2\n",
    "merged_df['scope_1_revenue_part'] = merged_df['revenue'] * merged_df['revenue_pct'] * merged_df['affects_scope_1'].astype(int)\n",
    "merged_df['scope_2_revenue_part'] = merged_df['revenue'] * merged_df['revenue_pct'] * merged_df['affects_scope_2'].astype(int)\n",
    "\n",
    "# Aggregate by entity\n",
    "entity_revenue_split = merged_df.groupby('entity_id')[['scope_1_revenue_part', 'scope_2_revenue_part']].sum().reset_index()\n",
    "entity_revenue_split.rename(columns={'scope_1_revenue_part': 'scope_1_revenue', 'scope_2_revenue_part': 'scope_2_revenue'}, inplace=True)\n",
    "\n",
    "final_df = pd.merge(df, entity_revenue_split, on='entity_id', how='left').fillna(0)\n",
    "\n",
    "# 1. Plot Scope 1 / Scope 2 Revenue Distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "sns.histplot(final_df['scope_1_revenue'], kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Scope 1 Revenue (Raw)')\n",
    "\n",
    "sns.histplot(np.log1p(final_df['scope_1_revenue']), kde=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Scope 1 Revenue (Log)')\n",
    "\n",
    "sns.histplot(final_df['scope_2_revenue'], kde=True, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Scope 2 Revenue (Raw)')\n",
    "\n",
    "sns.histplot(np.log1p(final_df['scope_2_revenue']), kde=True, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Scope 2 Revenue (Log)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Calculate Log Correlations (Filtered for non-zero revenue)\n",
    "final_df['log_scope_1_revenue'] = np.log1p(final_df['scope_1_revenue'])\n",
    "final_df['log_scope_2_revenue'] = np.log1p(final_df['scope_2_revenue'])\n",
    "final_df['log_target_scope_1'] = np.log1p(final_df['target_scope_1'])\n",
    "final_df['log_target_scope_2'] = np.log1p(final_df['target_scope_2'])\n",
    "\n",
    "# Filter for non-zero scope 1 revenue\n",
    "df_s1 = final_df[final_df['scope_1_revenue'] > 0]\n",
    "corr_s1 = df_s1['log_scope_1_revenue'].corr(df_s1['log_target_scope_1'])\n",
    "\n",
    "# Filter for non-zero scope 2 revenue\n",
    "df_s2 = final_df[final_df['scope_2_revenue'] > 0]\n",
    "corr_s2 = df_s2['log_scope_2_revenue'].corr(df_s2['log_target_scope_2'])\n",
    "\n",
    "print(f\"Correlation (Filtered > 0): Log(Scope 1 Revenue) vs Log(Scope 1 Emissions): {corr_s1:.4f}\")\n",
    "print(f\"Correlation (Filtered > 0): Log(Scope 2 Revenue) vs Log(Scope 2 Emissions): {corr_s2:.4f}\")\n",
    "\n",
    "# 3. Report Company Counts\n",
    "has_s1 = (final_df['scope_1_revenue'] > 0).sum()\n",
    "has_s2 = (final_df['scope_2_revenue'] > 0).sum()\n",
    "has_both = ((final_df['scope_1_revenue'] > 0) & (final_df['scope_2_revenue'] > 0)).sum()\n",
    "has_neither = ((final_df['scope_1_revenue'] == 0) & (final_df['scope_2_revenue'] == 0)).sum()\n",
    "\n",
    "print(f\"Companies with Scope 1 Revenue: {has_s1}\")\n",
    "print(f\"Companies with Scope 2 Revenue: {has_s2}\")\n",
    "print(f\"Companies with Both: {has_both}\")\n",
    "print(f\"Companies with NO material sector revenue (Both False): {has_neither}\")\n",
    "\n",
    "# 4. Box Plots for Materiality Groups\n",
    "final_df['materiality_group'] = 'Any Material'\n",
    "final_df.loc[(final_df['scope_1_revenue'] == 0) & (final_df['scope_2_revenue'] == 0), 'materiality_group'] = 'None'\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.boxplot(x='materiality_group', y='log_target_scope_1', data=final_df, ax=axes[0])\n",
    "axes[0].set_title('Log(Scope 1 Emissions) by Materiality')\n",
    "\n",
    "sns.boxplot(x='materiality_group', y='log_target_scope_2', data=final_df, ax=axes[1])\n",
    "axes[1].set_title('Log(Scope 2 Emissions) by Materiality')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fbb002",
   "metadata": {},
   "source": [
    "## 6. Geography\n",
    "\n",
    "Geographic context can materially affect emissions through grid carbon intensity, regulatory regimes, and typical industrial practices. This section summarizes observations and operational decisions regarding geographic features.\n",
    "\n",
    "Observed patterns\n",
    "\n",
    "- The training data is concentrated in a few regions (notably `WEU` and `NAM`); many other regions have sparse representation.\n",
    "- The test set composition is similar, which reduces the benefit of highly granular region encodings for generalization.\n",
    "\n",
    "Practical recommendations\n",
    "\n",
    "- Collapse minor regions into an `OTHER` category; retain `WEU` and `NAM` explicitly where they dominate.\n",
    "- Keep `country_code` as a categorical predictor when sample sizes permit; encode with regularization to avoid high variance.\n",
    "- Document geographic encodings and monitor geographic performance slices to detect bias or unstable predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d226eaf",
   "metadata": {},
   "source": [
    "## 6.1 Hypothesis 3\n",
    "\n",
    "> For companies with comparable scale and sector exposure, country-level factors will explain residual differences in emissions; this effect is expected to be stronger for Scope 2 than Scope 1.\n",
    "\n",
    "Analytical approach\n",
    "\n",
    "- Control for scale using revenue quantiles and compare country distributions within bins.\n",
    "- Use both absolute and log-transformed targets to quantify multiplicative and additive differences.\n",
    "- Apply Welch’s t-test for mean differences and report multiplicative ratios in log space where informative.\n",
    "\n",
    "Expected outcomes and implications\n",
    "\n",
    "- Scope 1: modest country-level differences (expected order-of-magnitude: single-digit percent differences).\n",
    "- Scope 2: larger differences driven by grid mix; multiplicative factors can be substantial.\n",
    "\n",
    "Operational guidance\n",
    "\n",
    "- Use country as a regularized categorical feature; avoid overfitting by collapsing countries with insufficient samples.\n",
    "- When deploying models, generate country/regional diagnostic reports to ensure equitable and stable performance across geographies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c65c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "def analyze_country_factors(df, scope_col, revenue_col, country_col='country_code', \n",
    "                            n_quantiles=10, use_log_target=False):\n",
    "    \"\"\"\n",
    "    use_log_target=False → 기존 raw target 분석\n",
    "    use_log_target=True  → log1p(target) 기반 안정화된 factor 분석\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Create log target column if needed\n",
    "    if use_log_target:\n",
    "        scope_col_used = f\"log_{scope_col}\"\n",
    "        df[scope_col_used] = np.log1p(df[scope_col])\n",
    "        print(f\"\\n=== Log-Transformed Analysis for {scope_col} vs {revenue_col} ===\")\n",
    "    else:\n",
    "        scope_col_used = scope_col\n",
    "        print(f\"\\n=== Analysis for {scope_col} vs {revenue_col} ===\")\n",
    "    \n",
    "    # Step A: Filter and Quantiles\n",
    "    df_valid = df[df[revenue_col] > 0].copy()\n",
    "    df_valid['log_revenue'] = np.log1p(df_valid[revenue_col])\n",
    "    \n",
    "    # Quantile binning\n",
    "    try:\n",
    "        df_valid['quantile'] = pd.qcut(df_valid['log_revenue'], n_quantiles, labels=False)\n",
    "    except ValueError:\n",
    "        print(\"Warning: Not enough unique revenue values for quantiles. Using rank-based binning.\")\n",
    "        df_valid['quantile'] = pd.qcut(df_valid['log_revenue'].rank(method='first'), n_quantiles, labels=False)\n",
    "    \n",
    "    # Step B-D: Per group analysis\n",
    "    for q in range(n_quantiles):\n",
    "        group_df = df_valid[df_valid['quantile'] == q]\n",
    "        if group_df.empty:\n",
    "            continue\n",
    "        \n",
    "        country_counts = group_df[country_col].value_counts()\n",
    "        if country_counts.empty:\n",
    "            continue\n",
    "        \n",
    "        baseline_country = country_counts.idxmax()\n",
    "        baseline_data = group_df[group_df[country_col] == baseline_country][scope_col_used]\n",
    "        \n",
    "        # Stats\n",
    "        b_min = baseline_data.min()\n",
    "        b_max = baseline_data.max()\n",
    "        b_mean = baseline_data.mean()\n",
    "        b_std = baseline_data.std()\n",
    "        b_median = baseline_data.median()\n",
    "        \n",
    "        print(f\"\\n[Group {q} (Quantile {q+1}/{n_quantiles})]\")\n",
    "        print(f\"- Baseline country: {baseline_country} (n={len(baseline_data)})\")\n",
    "        print(f\"- Baseline stats: min={b_min:.4f}, max={b_max:.4f}, mean={b_mean:.4f}, std={b_std:.4f}, median={b_median:.4f}\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for country in country_counts.index:\n",
    "            if country == baseline_country:\n",
    "                continue\n",
    "            \n",
    "            c_data = group_df[group_df[country_col] == country][scope_col_used]\n",
    "            if len(c_data) < 2:\n",
    "                continue\n",
    "            \n",
    "            c_mean = c_data.mean()\n",
    "            c_std = c_data.std()\n",
    "            \n",
    "            # Ratio changes if using log or raw\n",
    "            if use_log_target:\n",
    "                # In log space: exp(mean difference) gives multiplicative factor\n",
    "                mean_ratio = np.exp(c_mean - b_mean)\n",
    "            else:\n",
    "                mean_ratio = c_mean / b_mean if b_mean != 0 else np.nan\n",
    "            \n",
    "            # Welch test\n",
    "            t_stat, p_val = stats.ttest_ind(c_data, baseline_data, equal_var=False)\n",
    "            is_sig = \"Yes\" if p_val < 0.05 else \"No\"\n",
    "            \n",
    "            results.append({\n",
    "                'Country': country,\n",
    "                'n': len(c_data),\n",
    "                'Mean': c_mean,\n",
    "                'Std': c_std,\n",
    "                'Ratio': mean_ratio,\n",
    "                'p-value': p_val,\n",
    "                'Sig?': is_sig\n",
    "            })\n",
    "        \n",
    "        if results:\n",
    "            print(pd.DataFrame(results).to_string(index=False, float_format=lambda x: \"{:.4f}\".format(x)))\n",
    "        else:\n",
    "            print(\"No other countries with sufficient data for comparison.\")\n",
    "\n",
    "\n",
    "# Run Analysis for Scope 1\n",
    "# Note: Using 'scope_1_revenue' calculated in previous step\n",
    "analyze_country_factors(final_df, 'target_scope_1', 'scope_1_revenue')\n",
    "\n",
    "# Run Analysis for Scope 2\n",
    "# Note: Using 'scope_2_revenue' calculated in previous step\n",
    "analyze_country_factors(final_df, 'target_scope_2', 'scope_2_revenue')\n",
    "\n",
    "# Run Analysis for Log Scope 1\n",
    "analyze_country_factors(final_df, 'target_scope_1', 'scope_1_revenue', use_log_target=True)\n",
    "\n",
    "# Run Analysis for Log Scope 2\n",
    "analyze_country_factors(final_df, 'target_scope_2', 'scope_2_revenue', use_log_target=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62d3258",
   "metadata": {},
   "source": [
    "### Technical Conclusions — ESG and Adjusted Scores\n",
    "\n",
    "Key findings\n",
    "\n",
    "- The adjusted environmental score (environmental_score + aggregated activity adjustments) exhibits more consistent correlation with emissions than the raw environmental score alone.\n",
    "- `overall_score` is a weighted sum of E/S/G components and can create multicollinearity if included simultaneously with its constituents.\n",
    "\n",
    "Recommendations\n",
    "\n",
    "- Use the adjusted environmental score (`env_adjusted`) in models instead of `overall_score`.\n",
    "- Include `social_score` and `governance_score` as separate features if they provide incremental signal, but monitor multicollinearity.\n",
    "- Prefer non-linear models (e.g., gradient-boosted trees) to capture interactions between ESG adjustments and scale/sector features.\n",
    "\n",
    "Implementation notes\n",
    "\n",
    "- Aggregate environmental activity adjustments at the company level (sum of `env_score_adjustment`) and add to the base `environmental_score`.\n",
    "- Standardize or clip ESG features consistently across train/test datasets to avoid distribution drift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba35d5bc",
   "metadata": {},
   "source": [
    "# 7. Behavioral Features\n",
    "\n",
    "Not yet fully explored.\n",
    "\n",
    "Potential signals:\n",
    "\n",
    "Negative environmental adjustments indicate beneficial activities.\n",
    "\n",
    "SDG commitments (especially SDG 13 “Climate Action”).\n",
    "\n",
    "ESG scores may reflect process maturity but can be noisy.\n",
    "\n",
    "Further EDA is required to validate their predictive value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a6bb3d",
   "metadata": {},
   "source": [
    "# 7.1 overall_score & e/s/g score\n",
    "- overall = 0.45 * e + 0.3 * s + 0.25 * g => exclude overall score as a feature due to multicolinearity\n",
    "- 1 to 5 scale. closer to 1 the better\n",
    "- data/environmental_activities.csv: e score adjustment. simple addition? (additive adjustment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a12bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load environmental activities\n",
    "env_activities = pd.read_csv('data/environmental_activities.csv')\n",
    "\n",
    "# Aggregate adjustments by entity_id\n",
    "env_adj_agg = env_activities.groupby('entity_id')['env_score_adjustment'].sum().reset_index()\n",
    "\n",
    "# Merge with final_df\n",
    "# We use left merge to keep all companies in final_df, filling missing adjustments with 0\n",
    "analysis_df = final_df.merge(env_adj_agg, on='entity_id', how='left')\n",
    "analysis_df['env_score_adjustment'] = analysis_df['env_score_adjustment'].fillna(0)\n",
    "\n",
    "# Calculate adjusted scores\n",
    "analysis_df['env_adjusted'] = analysis_df['environmental_score'] + analysis_df['env_score_adjustment']\n",
    "analysis_df['overall_adjusted'] = (0.45 * analysis_df['env_adjusted'] + \n",
    "                                   0.3 * analysis_df['social_score'] + \n",
    "                                   0.25 * analysis_df['governance_score'])\n",
    "\n",
    "# Create log targets\n",
    "analysis_df['log_target_scope_1'] = np.log1p(analysis_df['target_scope_1'])\n",
    "analysis_df['log_target_scope_2'] = np.log1p(analysis_df['target_scope_2'])\n",
    "\n",
    "# === Global Correlations (No Quantiles) ===\n",
    "global_targets = [\n",
    "    ('Scope 1 Target', 'target_scope_1'),\n",
    "    ('Log Scope 1 Target', 'log_target_scope_1'),\n",
    "    ('Scope 2 Target', 'target_scope_2'),\n",
    "    ('Log Scope 2 Target', 'log_target_scope_2')\n",
    "]\n",
    "\n",
    "score_cols_map = {\n",
    "    'overall_score': 'overall',\n",
    "    'overall_adjusted': 'overall_adjusted',\n",
    "    'environmental_score': 'env',\n",
    "    'env_adjusted': 'env_adjusted',\n",
    "    'social_score': 'social',\n",
    "    'governance_score': 'governance'\n",
    "}\n",
    "\n",
    "global_results = []\n",
    "for label, t_col in global_targets:\n",
    "    row = {'target': label}\n",
    "    for s_col, s_label in score_cols_map.items():\n",
    "        # Calculate correlation on the full dataset (ignoring NaNs)\n",
    "        row[s_label] = analysis_df[t_col].corr(analysis_df[s_col])\n",
    "    global_results.append(row)\n",
    "\n",
    "global_corr_df = pd.DataFrame(global_results)\n",
    "# Ensure column order\n",
    "global_cols = ['target', 'overall', 'overall_adjusted', 'env', 'env_adjusted', 'social', 'governance']\n",
    "global_corr_df = global_corr_df[global_cols]\n",
    "\n",
    "print(\"=== Global Correlations (All Data) ===\")\n",
    "print(global_corr_df.to_string(index=False, float_format=lambda x: \"{:.4f}\".format(x)))\n",
    "print(\"\\n\")\n",
    "\n",
    "# === Quantile Analysis ===\n",
    "def calculate_quantile_correlations(df, revenue_col, target_col, title):\n",
    "    score_cols = ['overall_score', 'overall_adjusted', 'environmental_score', 'env_adjusted', 'social_score', 'governance_score']\n",
    "    \n",
    "    # Filter for valid revenue\n",
    "    valid_df = df[df[revenue_col] > 0].copy()\n",
    "    \n",
    "    # Create quantiles\n",
    "    n_quantiles = 10\n",
    "    try:\n",
    "        valid_df['quantile'] = pd.qcut(valid_df[revenue_col], n_quantiles, labels=False)\n",
    "    except ValueError:\n",
    "        # Fallback if not enough unique values\n",
    "        valid_df['quantile'] = pd.qcut(valid_df[revenue_col].rank(method='first'), n_quantiles, labels=False)\n",
    "    \n",
    "    # Calculate correlations per group\n",
    "    results = []\n",
    "    for q in range(n_quantiles):\n",
    "        group_data = valid_df[valid_df['quantile'] == q]\n",
    "        if len(group_data) < 2:\n",
    "            continue\n",
    "        \n",
    "        row = {'quantile group': q}\n",
    "        for col in score_cols:\n",
    "            corr = group_data[col].corr(group_data[target_col])\n",
    "            row[col] = corr\n",
    "        results.append(row)\n",
    "    \n",
    "    # Create result dataframe\n",
    "    corr_table = pd.DataFrame(results)\n",
    "    \n",
    "    # Rename columns for display\n",
    "    corr_table = corr_table.rename(columns={\n",
    "        'overall_score': 'overall',\n",
    "        'environmental_score': 'env',\n",
    "        'social_score': 'social',\n",
    "        'governance_score': 'governance'\n",
    "    })\n",
    "    \n",
    "    # Reorder columns\n",
    "    display_cols = ['quantile group', 'overall', 'overall_adjusted', 'env', 'env_adjusted', 'social', 'governance']\n",
    "    corr_table = corr_table[display_cols]\n",
    "    \n",
    "    print(f\"=== {title} ===\")\n",
    "    print(corr_table.to_string(index=False, float_format=lambda x: \"{:.4f}\".format(x)))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# 1. Scope 1 Revenue vs Target Scope 1\n",
    "calculate_quantile_correlations(analysis_df, 'scope_1_revenue', 'target_scope_1', \n",
    "                              'Correlation: Scope 1 Revenue Quantile vs Target Scope 1')\n",
    "\n",
    "# 2. Scope 1 Revenue vs Log Target Scope 1\n",
    "calculate_quantile_correlations(analysis_df, 'scope_1_revenue', 'log_target_scope_1', \n",
    "                              'Correlation: Scope 1 Revenue Quantile vs Log Target Scope 1')\n",
    "\n",
    "# 3. Scope 2 Revenue vs Target Scope 2\n",
    "calculate_quantile_correlations(analysis_df, 'scope_2_revenue', 'target_scope_2', \n",
    "                              'Correlation: Scope 2 Revenue Quantile vs Target Scope 2')\n",
    "\n",
    "# 4. Scope 2 Revenue vs Log Target Scope 2\n",
    "calculate_quantile_correlations(analysis_df, 'scope_2_revenue', 'log_target_scope_2', \n",
    "                              'Correlation: Scope 2 Revenue Quantile vs Log Target Scope 2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a53239",
   "metadata": {},
   "source": [
    "- 산업 규모를 무시하고 보면 correlation 작아보임\n",
    "- 산업 규모(revenue)를 고려하면 그룹마다 correlation 꽤 크게 보이는 경우 있음\n",
    "- 음수/양수 들쭉날쭉 해서 비선형적인 관계겠지만 사용하기에 좋은 피쳐라는 뜻. 어차피 산업 규모(revenue)도 피쳐로 들어가기 때문.\n",
    "- 비선형적 모델 ex) XGBoost 를 쓸때 도움될 것\n",
    "- 그리고 overall vs overall adjusted, env vs env adjusted 비교해봤을때, adjusted score가 consistent하게 더 correlation이 크기에 더 좋은 피쳐다\n",
    "- overall은 e/s/g score의 weighted sum이라 multicolinearity를 유발해서 쓰지 말자\n",
    "\n",
    "결론: esg score feature를 사용하되, env는 adjusted로 써야 한다, overall score는 쓰지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c25d8a",
   "metadata": {},
   "source": [
    "# 7.2 sdg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sdg_analysis_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Load SDG data\n",
    "sdg_df = pd.read_csv('data/sustainable_development_goals.csv')\n",
    "unique_sdgs = sdg_df[['sdg_id', 'sdg_name']].drop_duplicates().sort_values('sdg_id')\n",
    "\n",
    "# Targets to analyze\n",
    "targets = [\n",
    "    ('target_scope_1', 'target scope 1'),\n",
    "    ('log_target_scope_1', 'target scope 1 log'),\n",
    "    ('target_scope_2', 'target scope 2'),\n",
    "    ('log_target_scope_2', 'target scope 2 log')\n",
    "]\n",
    "\n",
    "significant_cases = []\n",
    "\n",
    "for _, row in unique_sdgs.iterrows():\n",
    "    sdg_id = row['sdg_id']\n",
    "    sdg_name = row['sdg_name']\n",
    "    \n",
    "    print(f\"[======={sdg_name} ({sdg_id})=======]\")\n",
    "    \n",
    "    # Identify entities with this SDG\n",
    "    entities_with_sdg = sdg_df[sdg_df['sdg_id'] == sdg_id]['entity_id'].unique()\n",
    "    \n",
    "    has_sdg = analysis_df['entity_id'].isin(entities_with_sdg)\n",
    "    group_exist = analysis_df[has_sdg]\n",
    "    group_non_exist = analysis_df[~has_sdg]\n",
    "    \n",
    "    # Global comparison table\n",
    "    print(\"|target | N(exist) | N(non-exist) | existence mean | non existence mean | p-value | is significant|\")\n",
    "    for col, label in targets:\n",
    "        a = group_exist[col].dropna()\n",
    "        b = group_non_exist[col].dropna()\n",
    "        \n",
    "        n_exist = len(a)\n",
    "        n_non_exist = len(b)\n",
    "        \n",
    "        if n_exist > 1 and n_non_exist > 1:\n",
    "            stat, pval = stats.ttest_ind(a, b, equal_var=False)\n",
    "            mean_exist = a.mean()\n",
    "            mean_non_exist = b.mean()\n",
    "            is_sig = pval < 0.05\n",
    "        else:\n",
    "            mean_exist = np.nan\n",
    "            mean_non_exist = np.nan\n",
    "            pval = np.nan\n",
    "            is_sig = False\n",
    "            \n",
    "        print(f\"|{label}| {n_exist} | {n_non_exist} | {mean_exist:.4f} | {mean_non_exist:.4f} | {pval:.4f} | {is_sig}|\")\n",
    "        \n",
    "        if is_sig:\n",
    "            significant_cases.append({\n",
    "                'type': 'Global',\n",
    "                'sdg': f\"{sdg_name} ({sdg_id})\",\n",
    "                'target': label,\n",
    "                'quantile': 'All',\n",
    "                'p_value': pval,\n",
    "                'mean_diff': mean_exist - mean_non_exist\n",
    "            })\n",
    "            \n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Quantile Analysis Helper\n",
    "    def run_quantile_analysis(revenue_col, target_col, target_label, quantile_label):\n",
    "        print(f\"[{quantile_label}, {target_label} mean comparison]\")\n",
    "        print(\"|quantile group| N(exist) | N(non-exist) | existence mean | non existence mean | p-value | is significant|\")\n",
    "        \n",
    "        temp_df = analysis_df[analysis_df[revenue_col] > 0].copy()\n",
    "        \n",
    "        try:\n",
    "            temp_df['quantile'] = pd.qcut(temp_df[revenue_col], 10, labels=False)\n",
    "        except ValueError:\n",
    "             temp_df['quantile'] = pd.qcut(temp_df[revenue_col].rank(method='first'), 10, labels=False)\n",
    "             \n",
    "        for q in range(10):\n",
    "            q_data = temp_df[temp_df['quantile'] == q]\n",
    "            \n",
    "            has_sdg_q = q_data['entity_id'].isin(entities_with_sdg)\n",
    "            g_exist = q_data[has_sdg_q][target_col].dropna()\n",
    "            g_non_exist = q_data[~has_sdg_q][target_col].dropna()\n",
    "            \n",
    "            n_exist = len(g_exist)\n",
    "            n_non_exist = len(g_non_exist)\n",
    "            \n",
    "            if n_exist > 1 and n_non_exist > 1:\n",
    "                stat, pval = stats.ttest_ind(g_exist, g_non_exist, equal_var=False)\n",
    "                m_exist = g_exist.mean()\n",
    "                m_non_exist = g_non_exist.mean()\n",
    "                is_sig = pval < 0.05\n",
    "            else:\n",
    "                m_exist = np.nan\n",
    "                m_non_exist = np.nan\n",
    "                pval = np.nan\n",
    "                is_sig = False\n",
    "            \n",
    "            print(f\"|{q}| {n_exist} | {n_non_exist} | {m_exist:.4f} | {m_non_exist:.4f} | {pval:.4f} | {is_sig}|\")\n",
    "            \n",
    "            if is_sig:\n",
    "                significant_cases.append({\n",
    "                    'type': 'Quantile',\n",
    "                    'sdg': f\"{sdg_name} ({sdg_id})\",\n",
    "                    'target': target_label,\n",
    "                    'quantile': f\"{quantile_label} (Group {q})\",\n",
    "                    'p_value': pval,\n",
    "                    'mean_diff': m_exist - m_non_exist\n",
    "                })\n",
    "        print(\"\\n\")\n",
    "\n",
    "    run_quantile_analysis('scope_1_revenue', 'target_scope_1', 'target scope 1', 'scope 1 revenue quantile')\n",
    "    run_quantile_analysis('scope_1_revenue', 'log_target_scope_1', 'target scope 1 log', 'scope 1 revenue quantile')\n",
    "    run_quantile_analysis('scope_2_revenue', 'target_scope_2', 'target scope 2', 'scope 2 revenue quantile')\n",
    "    run_quantile_analysis('scope_2_revenue', 'log_target_scope_2', 'target scope 2 log', 'scope 2 revenue quantile')\n",
    "    \n",
    "    print(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "print(\"=== Summary of Significant Cases ===\")\n",
    "if significant_cases:\n",
    "    sig_df = pd.DataFrame(significant_cases)\n",
    "    # Sort by p-value for better readability\n",
    "    sig_df = sig_df.sort_values('p_value')\n",
    "    print(sig_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No significant cases found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e044a74",
   "metadata": {},
   "source": [
    "- SDG는 예측 신호가 아니다\n",
    "- Global에서 유의미해도 규모로 통제하면 사라짐. 즉, “기업 규모가 매우 작은 기업들만 SDG를 가진 경향” 같은 confounding effect일 가능성.\n",
    "- SDG는 자기보고(self-reported)이며 실제 운영과 먼 경우가 많음\n",
    "- 표본이 너무 적음\n",
    "- 따라서 피쳐로 사용하지 않는다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6802dfb1",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "To validate whether the insights from the earlier data analysis actually improve prediction of Scope 1 and Scope 2 emissions, several experiments were conducted. Each experiment combines a preprocessing strategy with a specific model type. The goal is to compare both datasets and modeling approaches in a controlled setting.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Two dataset configurations are compared.\n",
    "\n",
    "### 1. Baseline Dataset\n",
    "This follows the preprocessing pipeline originally provided by the Codeathon organizers. It includes:\n",
    "\n",
    "- One-hot encoding of `region`  \n",
    "- Adding sector-level revenue share using the revenue distribution table  \n",
    "- Adding environmental activity score adjustments  \n",
    "- Adding binary features representing Sustainable Development Goal (SDG) activities  \n",
    "- Applying a variance threshold of 0.05 for feature selection  \n",
    "\n",
    "### 2. Proposed Dataset\n",
    "This dataset uses features identified through exploratory analysis and domain reasoning. The feature set includes:\n",
    "\n",
    "- `log_total_revenue`\n",
    "- `log_scope1_revenue`\n",
    "- `log_scope2_revenue`\n",
    "- `scope1_revenue_present` (0 or 1)\n",
    "- `scope2_revenue_present` (0 or 1)\n",
    "- `env_adjusted_score`\n",
    "- `social_score`\n",
    "- `governance_score`\n",
    "- `country_code`  \n",
    "  - Used directly as a categorical variable for tree-based models  \n",
    "  - One-hot encoded for linear models  \n",
    "\n",
    "The target variables (`target_scope_1`, `target_scope_2`) are log-transformed during training to stabilize variance. Predictions are converted back to raw scale for final evaluation.\n",
    "\n",
    "\n",
    "## Models\n",
    "\n",
    "Three different models are used to evaluate linear trends, robustness to outliers, and nonlinear relationships.\n",
    "\n",
    "### 1. Linear Regression\n",
    "This is the baseline model defined by the organizers. It optimizes MSE and serves as a reference point. Linear regression is sensitive to outliers and cannot capture nonlinearity.\n",
    "\n",
    "### 2. Median Regression\n",
    "Our primary metric is MAE, and secondary metrics are log MAE and log RMSE. Since all emphasize central tendency and robustness, median regression is appropriate. The regularization constant (alpha) is set to 0 to avoid biasing the solution.\n",
    "\n",
    "### 3. CatBoost\n",
    "Given the skewed numerical features and categorical variables such as `country_code`, a tree-based model is well suited. CatBoost handles categorical encoding internally and captures nonlinear effects efficiently, making it a strong candidate for small datasets.\n",
    "\n",
    "\n",
    "## Experiment Settings\n",
    "\n",
    "We evaluate four combinations of dataset and model:\n",
    "\n",
    "1. **Baseline Dataset + Linear Regression**  \n",
    "   This recreates the original baseline setup and serves as a comparison anchor.\n",
    "\n",
    "2. **Baseline Dataset + Median Regression**  \n",
    "   This tests whether replacing the baseline model with a MAE-oriented model improves performance under the same feature configuration.\n",
    "\n",
    "3. **Proposed Dataset + Median Regression**  \n",
    "   This evaluates whether the newly engineered features provide meaningful gains over the baseline features.\n",
    "\n",
    "4. **Proposed Dataset + CatBoost**  \n",
    "   This leverages a tree-based method to capture nonlinear patterns and directly handle categorical variables, aiming for the best overall performance.\n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "The following table summarizes the cross-validated performance of all four experiment settings.  \n",
    "Metrics reported are MAE, log-MAE, and log-RMSE for both Scope 1 and Scope 2 targets.\n",
    "\n",
    "### Final Results Table\n",
    "\n",
    "| Model                                      | Scope 1 MAE | Scope 1 Log MAE | Scope 1 Log RMSE | Scope 2 MAE | Scope 2 Log MAE | Scope 2 Log RMSE |\n",
    "|--------------------------------------------|-------------|-----------------|------------------|-------------|-----------------|------------------|\n",
    "| LinearRegression + Baseline                | 64409.1131  | 2.5848          | 3.5073           | 71708.9193  | 3.1736          | 4.3514           |\n",
    "| MedianRegression + Baseline                | 51830.9031  | 2.0765          | 2.8194           | 52615.5835  | 2.1804          | 3.2238           |\n",
    "| MedianRegression + NewFeatures + LogTarget | 51025.5594  | 1.5479          | 1.9553           | 55099.4216  | 1.8250          | 2.4967           |\n",
    "| CatBoost + NewFeatures + LogTarget         | **50094.6028** | **1.4946**     | **1.8922**       | **52235.8818** | **1.8224**     | **2.5350**       |\n",
    "\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "### 1. Baseline vs Median Regression  \n",
    "- Median regression significantly outperforms linear regression across all metrics.  \n",
    "- This confirms that MAE-oriented modeling and robustness to outliers are important for this task.\n",
    "\n",
    "### 2. Baseline Features vs Proposed Features  \n",
    "- Replacing the baseline features with the proposed analytical features further improves performance.  \n",
    "- Log-transforming the targets stabilizes heavy skewness and reduces both log-MAE and log-RMSE substantially.\n",
    "\n",
    "### 3. CatBoost Performance  \n",
    "- CatBoost delivers the best results for Scope 1 and competitive results for Scope 2.  \n",
    "- Its ability to handle nonlinearity and categorical variables directly gives it an advantage.  \n",
    "- Overall, **CatBoost + Proposed Features + Log Target** is the top-performing setup.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "- Feature engineering based on domain insights leads to consistent improvements.  \n",
    "- Log-target modeling is highly effective for emission data with heavy-tailed distributions.  \n",
    "- CatBoost is the strongest model overall, indicating that nonlinear relationships and categorical structure play an important role in predicting emissions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfeb89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from src.preprocessor import (\n",
    "    CodeathonBaselinePreprocessor,\n",
    "    ProposedFeaturePreprocessor\n",
    ")\n",
    "from src.models import (\n",
    "    LinearRegressionModel,\n",
    "    MedianRegressionModel,\n",
    "    LogTargetCatBoostModel,\n",
    "    LogTargetMedianRegressionModel\n",
    ")\n",
    "from src.trainer import Trainer\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Load Data\n",
    "# -----------------------------------------------------\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "y_scope1 = train_df['target_scope_1']\n",
    "y_scope2 = train_df['target_scope_2']\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Define Experiment Configurations\n",
    "# -----------------------------------------------------\n",
    "experiments = [\n",
    "    {\n",
    "        \"name\": \"LinearRegression + Baseline\",\n",
    "        \"preprocessor\": CodeathonBaselinePreprocessor(),\n",
    "        \"model_class\": LinearRegressionModel,\n",
    "        \"model_params\": {},\n",
    "        \"trainer_params\": {\"n_splits\": 5, \"random_state\": 42},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MedianRegression + Baseline\",\n",
    "        \"preprocessor\": CodeathonBaselinePreprocessor(),\n",
    "        \"model_class\": MedianRegressionModel,\n",
    "        \"model_params\": {},\n",
    "        \"trainer_params\": {\"n_splits\": 5, \"random_state\": 42},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MedianRegression + NewFeatures + LogTarget\",\n",
    "        \"preprocessor\": ProposedFeaturePreprocessor(),\n",
    "        \"model_class\": LogTargetMedianRegressionModel,\n",
    "        \"model_params\": {},\n",
    "        \"trainer_params\": {\"n_splits\": 5, \"random_state\": 42},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"CatBoost + NewFeatures + LogTarget\",\n",
    "        \"preprocessor\": ProposedFeaturePreprocessor(tree=True),\n",
    "        \"model_class\": LogTargetCatBoostModel,\n",
    "        \"model_params\": {\n",
    "            'iterations': 100,\n",
    "            'depth': 6,\n",
    "            'learning_rate': 0.1,\n",
    "            'random_state': 42\n",
    "        },\n",
    "        \"trainer_params\": {\"n_splits\": 5, \"random_state\": 42},\n",
    "    },\n",
    "]\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Run Experiments\n",
    "# -----------------------------------------------------\n",
    "results = []\n",
    "\n",
    "for exp in experiments:\n",
    "    print(f\"\\n=== Running {exp['name']} ===\")\n",
    "\n",
    "    # Preprocess\n",
    "    X = exp[\"preprocessor\"].fit_transform(train_df)\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model_class=exp[\"model_class\"],\n",
    "        model_params=exp[\"model_params\"],\n",
    "        **exp[\"trainer_params\"]\n",
    "    )\n",
    "\n",
    "    # Train scope 1\n",
    "    m1 = trainer.train(X, y_scope1)\n",
    "    # Train scope 2\n",
    "    m2 = trainer.train(X, y_scope2)\n",
    "\n",
    "    # Save results into table row\n",
    "    results.append({\n",
    "        \"model\": exp[\"name\"],\n",
    "        \"scope1_mae\": m1[\"mean_mae\"],\n",
    "        \"scope1_log_mae\": m1[\"mean_log_mae\"],\n",
    "        \"scope1_log_rmse\": m1[\"mean_log_rmse\"],\n",
    "        \"scope2_mae\": m2[\"mean_mae\"],\n",
    "        \"scope2_log_mae\": m2[\"mean_log_mae\"],\n",
    "        \"scope2_log_rmse\": m2[\"mean_log_rmse\"],\n",
    "    })\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Results DataFrame\n",
    "# -----------------------------------------------------\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\\n==================== Final Results Table ====================\")\n",
    "print(\n",
    "    tabulate(\n",
    "        results_df,\n",
    "        headers=\"keys\",\n",
    "        tablefmt=\"github\",\n",
    "        floatfmt=\".4f\",\n",
    "        showindex=False\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d791d1d",
   "metadata": {},
   "source": [
    "# CatBoost Hyperparameter Optimization\n",
    "\n",
    "This section runs a lightweight hyperparameter search to improve the CatBoost model using the proposed feature set and log-transformed targets.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- The proposed feature preprocessor is applied once (`tree=True`) and reused.\n",
    "- A simple **random search** with 20 trials is performed.\n",
    "- Each trial trains two CatBoost models (Scope 1, Scope 2) using 5-fold CV.\n",
    "- Optimization target is the average MAE across both scopes.\n",
    "\n",
    "## Search Space\n",
    "\n",
    "- `iterations`: 200–1000  \n",
    "- `depth`: 4–8  \n",
    "- `learning_rate`: 0.03–0.1  \n",
    "- `l2_leaf_reg`: 1–10  \n",
    "- `random_state`: 42  \n",
    "\n",
    "## Process\n",
    "\n",
    "For each trial:\n",
    "\n",
    "1. Sample hyperparameters  \n",
    "2. Train CatBoost (log-target mode)  \n",
    "3. Compute MAE for Scope 1 and Scope 2  \n",
    "4. Calculate `(MAE1 + MAE2) / 2`  \n",
    "5. Record the result  \n",
    "\n",
    "The best configuration is chosen by sorting on the combined MAE.\n",
    "\n",
    "## Final Step\n",
    "\n",
    "- Retrain CatBoost with the best hyperparameters  \n",
    "- Add results as a new row:  \n",
    "  **\"CatBoost + NewFeatures + LogTarget (Optimized)\"**  \n",
    "- Append to the previous experiment table\n",
    "\n",
    "\n",
    "# Results (Including Optimized CatBoost)\n",
    "\n",
    "### Final Results Table (With Optimization)\n",
    "\n",
    "| Model                                          | Scope 1 MAE | Scope 1 Log MAE | Scope 1 Log RMSE | Scope 2 MAE | Scope 2 Log MAE | Scope 2 Log RMSE |\n",
    "|------------------------------------------------|-------------|-----------------|------------------|-------------|-----------------|------------------|\n",
    "| LinearRegression + Baseline                    | 64409.1131  | 2.5848          | 3.5073           | 71708.9193  | 3.1736          | 4.3514           |\n",
    "| MedianRegression + Baseline                    | 51830.9031  | 2.0765          | 2.8194           | 52615.5835  | 2.1804          | 3.2238           |\n",
    "| MedianRegression + NewFeatures + LogTarget     | 51025.5594  | 1.5479          | 1.9553           | 55099.4216  | 1.8250          | 2.4967           |\n",
    "| CatBoost + NewFeatures + LogTarget             | 50094.6028  | **1.4946**      | 1.8922           | **52235.8818** | 1.8224        | 2.5350           |\n",
    "| **CatBoost + NewFeatures + LogTarget (Optimized)** | **49595.7693** | 1.4977      | **1.8918**       | 53014.5223  | **1.8212**      | **2.5132**       |\n",
    "\n",
    "### Best Values by Metric\n",
    "\n",
    "- **Scope 1 MAE:** Optimized CatBoost  \n",
    "- **Scope 1 Log MAE:** CatBoost (non-optimized)  \n",
    "- **Scope 1 Log RMSE:** Optimized CatBoost  \n",
    "- **Scope 2 MAE:** CatBoost (non-optimized)  \n",
    "- **Scope 2 Log MAE:** Optimized CatBoost  \n",
    "- **Scope 2 Log RMSE:** Optimized CatBoost  \n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "- CatBoost consistently outperforms all linear and median regression baselines.\n",
    "- Hyperparameter optimization **only provides minor improvement**, mainly in Scope 1 metrics.\n",
    "- The non-optimized CatBoost model already performs extremely well across all metrics.\n",
    "- Given the marginal gains and small dataset size, **extensive hyperparameter tuning is unnecessary** for this task.\n",
    "\n",
    "In conclusion, **CatBoost + Proposed Features + LogTarget** is sufficiently strong even without hyperparameter optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef15f335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from src.preprocessor import ProposedFeaturePreprocessor\n",
    "from src.models import LogTargetCatBoostModel\n",
    "from src.trainer import Trainer\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Settings\n",
    "# -----------------------------------------------------\n",
    "N_TRIALS = 20\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "y_scope1 = train_df[\"target_scope_1\"]\n",
    "y_scope2 = train_df[\"target_scope_2\"]\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Preprocess once (tree mode)\n",
    "# -----------------------------------------------------\n",
    "preprocessor = ProposedFeaturePreprocessor(tree=True)\n",
    "X = preprocessor.fit_transform(train_df)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Hyperparameter Search Space\n",
    "# -----------------------------------------------------\n",
    "def sample_params():\n",
    "    return {\n",
    "        \"iterations\": random.choice([200, 300, 500, 800, 1000]),\n",
    "        \"depth\": random.choice([4, 5, 6, 7, 8]),\n",
    "        \"learning_rate\": random.choice([0.03, 0.05, 0.07, 0.1]),\n",
    "        \"l2_leaf_reg\": random.choice([1, 3, 5, 7, 10]),\n",
    "        \"random_state\": RANDOM_STATE\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Optimization Loop\n",
    "# -----------------------------------------------------\n",
    "trial_results = []\n",
    "\n",
    "print(\"\\n=== CatBoost Hyperparameter Optimization ===\")\n",
    "\n",
    "for trial in range(1, N_TRIALS + 1):\n",
    "    params = sample_params()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model_class=LogTargetCatBoostModel,\n",
    "        model_params=params,\n",
    "        n_splits=5,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    m1 = trainer.train(X, y_scope1)\n",
    "    m2 = trainer.train(X, y_scope2)\n",
    "\n",
    "    combined_score = (m1[\"mean_mae\"] + m2[\"mean_mae\"]) / 2\n",
    "\n",
    "    trial_results.append({\n",
    "        \"trial\": trial,\n",
    "        **params,\n",
    "        \"scope1_mae\": m1[\"mean_mae\"],\n",
    "        \"scope2_mae\": m2[\"mean_mae\"],\n",
    "        \"combined\": combined_score\n",
    "    })\n",
    "\n",
    "    print(f\"Trial {trial}/{N_TRIALS}: combined MAE = {combined_score:.3f} | params = {params}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Results data frame\n",
    "# -----------------------------------------------------\n",
    "hpo_results_df = pd.DataFrame(trial_results).sort_values(\"combined\")\n",
    "\n",
    "print(\"\\n\\n==================== Optimization Results ====================\")\n",
    "print(\n",
    "    tabulate(\n",
    "        hpo_results_df,\n",
    "        headers=\"keys\",\n",
    "        tablefmt=\"github\",\n",
    "        floatfmt=\".4f\",\n",
    "        showindex=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Extract best params\n",
    "# -----------------------------------------------------\n",
    "best_row = hpo_results_df.iloc[0].to_dict()\n",
    "best_params = {\n",
    "    key: best_row[key]\n",
    "    for key in [\"iterations\", \"depth\", \"learning_rate\", \"l2_leaf_reg\", \"random_state\"]\n",
    "}\n",
    "\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(best_params)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Retrain CatBoost using best params\n",
    "# =====================================================\n",
    "trainer = Trainer(\n",
    "    model_class=LogTargetCatBoostModel,\n",
    "    model_params=best_params,\n",
    "    n_splits=5,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "m1_best = trainer.train(X, y_scope1)\n",
    "m2_best = trainer.train(X, y_scope2)\n",
    "\n",
    "# Row to append\n",
    "optimized_row = {\n",
    "    \"model\": \"CatBoost + NewFeatures + LogTarget (Optimized)\",\n",
    "    \"scope1_mae\": m1_best[\"mean_mae\"],\n",
    "    \"scope1_log_mae\": m1_best[\"mean_log_mae\"],\n",
    "    \"scope1_log_rmse\": m1_best[\"mean_log_rmse\"],\n",
    "    \"scope2_mae\": m2_best[\"mean_mae\"],\n",
    "    \"scope2_log_mae\": m2_best[\"mean_log_mae\"],\n",
    "    \"scope2_log_rmse\": m2_best[\"mean_log_rmse\"],\n",
    "}\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Append to previous cell's results_df\n",
    "# =====================================================\n",
    "try:\n",
    "    final_results_df = pd.concat(\n",
    "        [results_df, pd.DataFrame([optimized_row])],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    print(\"\\n\\n==================== Final Results Table (With Optimization) ====================\")\n",
    "    print(\n",
    "        tabulate(\n",
    "            final_results_df,\n",
    "            headers=\"keys\",\n",
    "            tablefmt=\"github\",\n",
    "            floatfmt=\".4f\",\n",
    "            showindex=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "except NameError:\n",
    "    print(\"\\n[WARNING] previous cell's results_df not found.\")\n",
    "    print(\"Optimized row:\")\n",
    "    print(optimized_row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a641656",
   "metadata": {},
   "source": [
    "# Final Prediction (submission.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6d2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessor import ProposedFeaturePreprocessor\n",
    "from src.models import LogTargetCatBoostModel\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"\\n--- Training Final Models and Generating Submission ---\")\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = ProposedFeaturePreprocessor(tree=True)\n",
    "\n",
    "# Fit on train and transform\n",
    "X = preprocessor.fit_transform(train_df)\n",
    "X_test = preprocessor.transform(test_df)\n",
    "\n",
    "# Extract targets\n",
    "y_scope1 = train_df['target_scope_1']\n",
    "y_scope2 = train_df['target_scope_2']\n",
    "\n",
    "# Model params (from final.ipynb:L49-L60)\n",
    "model_params = {\n",
    "    'iterations': 100,\n",
    "    'depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Train Scope 1 on full data\n",
    "print(\"Training Scope 1...\")\n",
    "model_s1 = LogTargetCatBoostModel(**model_params)\n",
    "model_s1.fit(X, y_scope1)\n",
    "s1_predictions = model_s1.predict(X_test)\n",
    "\n",
    "# Train Scope 2 on full data\n",
    "print(\"Training Scope 2...\")\n",
    "model_s2 = LogTargetCatBoostModel(**model_params)\n",
    "model_s2.fit(X, y_scope2)\n",
    "s2_predictions = model_s2.predict(X_test)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'entity_id': test_df['entity_id'],\n",
    "    's1_predictions': s1_predictions,\n",
    "    's2_predictions': s2_predictions\n",
    "})\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Submission saved to submission.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e60d9b",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
