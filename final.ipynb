{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad3c776",
   "metadata": {},
   "source": [
    "# 1. Target Definition\n",
    "\n",
    "> In this project, our objective is to predict **two ground-truth greenhouse gas emission values** for each company. These targets correspond to the operational emissions defined by the **Greenhouse Gas Protocol**, the most widely adopted global framework for carbon accounting.\n",
    "\n",
    "## **Scope 1 Emissions (Direct Emissions)**\n",
    "\n",
    "**Scope 1** represents *direct* greenhouse gas emissions released from sources that are **owned or controlled by the company**.\n",
    "\n",
    "Typical examples include:\n",
    "- Onsite fuel combustion (boilers, furnaces, generators)\n",
    "- Emissions from manufacturing or chemical processing equipment\n",
    "- Company-owned vehicle fleets\n",
    "- Fugitive emissions from industrial systems (e.g., refrigerants, methane leaks)\n",
    "\n",
    "These emissions primarily reflect the **physical intensity and operational scale** of the company’s core business activities.  \n",
    "Sectors such as manufacturing, mining, energy production, and heavy transportation tend to exhibit significantly higher Scope 1 levels.\n",
    "\n",
    "\n",
    "## **Scope 2 Emissions (Indirect Emissions From Purchased Energy)**\n",
    "\n",
    "**Scope 2** represents *indirect* emissions generated from the production of **purchased electricity, heat, or steam** that the company consumes.\n",
    "\n",
    "Examples of activities contributing to Scope 2 include:\n",
    "- Electricity used for manufacturing lines  \n",
    "- Powering large buildings or data centers  \n",
    "- Purchased heating or cooling for industrial or commercial facilities  \n",
    "\n",
    "Because these emissions originate outside the company (at the utility/power provider), they differ fundamentally from Scope 1.\n",
    "\n",
    "\n",
    "## **Why Predict Both?**\n",
    "\n",
    "Together, **Scope 1 + Scope 2** represent the company’s **operational emissions**, a key component of ESG reporting and climate risk analysis.\n",
    "\n",
    "This means:\n",
    "- Different features contribute to each target  \n",
    "- Sector exposure affects each scope differently  \n",
    "- Business scale (revenue) alone cannot explain both emission types  \n",
    "\n",
    "For these reasons, Scope 1 and Scope 2 require **separate modeling approaches** even though both are part of the overall emissions profile.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8bdd1f",
   "metadata": {},
   "source": [
    "# 2. Why These Predictions Matter\n",
    "\n",
    "Many companies do not publicly report emissions. Financial institutions, rating agencies, and corporate partners still need approximate estimates for:\n",
    "\n",
    "- ESG and sustainability assessments\n",
    "- Investment risk analysis, including transition risk\n",
    "- Portfolio-level carbon accounting\n",
    "- Supply-chain emission estimation\n",
    "- Regulatory reporting and compliance checks\n",
    "\n",
    "In practice, these predictions are used in aggregate or at portfolio scale, so stable **median accuracy** is more valuable precise outlier prediction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b02403",
   "metadata": {},
   "source": [
    "# 3. Target Distribution and ML Objective\n",
    "\n",
    "## 3.1 Observed Distribution\n",
    "\n",
    "Exploratory data analysis reveals that both **Scope 1** and **Scope 2** exhibit strong **right-skewed distributions** with substantial variability across companies.  \n",
    "Key observations include:\n",
    "\n",
    "- Both targets contain **heavy tails**, with a small number of very large industrial firms contributing disproportionately to overall emissions.  \n",
    "- The **scales differ significantly** between companies, reflecting diverse operational sizes and sector activities.  \n",
    "- After applying a **log transformation**, the distributions become much closer to a bell-shaped form, indicating that emissions scale multiplicatively rather than linearly.\n",
    "\n",
    "This pattern is typical for energy-use and emissions datasets, where operational intensity varies dramatically across industries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae4d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('data/train.csv')\n",
    "\n",
    "# Plot distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Scope 1\n",
    "sns.histplot(df['target_scope_1'], kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Target Scope 1 Distribution')\n",
    "\n",
    "# Scope 2\n",
    "sns.histplot(df['target_scope_2'], kde=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Target Scope 2 Distribution')\n",
    "\n",
    "# Log Scope 1\n",
    "sns.histplot(np.log1p(df['target_scope_1']), kde=True, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Log(Target Scope 1) Distribution')\n",
    "\n",
    "# Log Scope 2\n",
    "sns.histplot(np.log1p(df['target_scope_2']), kde=True, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Log(Target Scope 2) Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e7c8db",
   "metadata": {},
   "source": [
    "## 3.2 Why Median-Focused Prediction Is More Appropriate\n",
    "\n",
    "For this task, we emphasize stable median predictions rather than precise modeling of extreme outliers. The main reasons are:\n",
    "\n",
    "- **Extreme emission values** generally arise from specialized industrial assets such as power plants, refineries, or large-scale manufacturing facilities. These activities cannot be captured reliably without detailed operational disclosures, which are not available in this dataset.  \n",
    "- ESG analysts and financial institutions primarily assess emissions at the **portfolio or sector level**, where stable central tendencies matter more than exact outlier accuracy.  \n",
    "- Loss functions like **RMSE** can overemphasize a small number of extreme values as it's more sensitive to outliers, resulting in unstable or biased models.  \n",
    "- Metrics such as **MAE**, **Log-MAE**, and **Log-RMSE** align more naturally with the distributional structure by focusing on the bulk of the data.\n",
    "\n",
    "Therefore, a median-oriented modeling approach produces more robust and practically useful predictions.\n",
    "\n",
    "## 3.3 Objective and Evaluation Metrics\n",
    "\n",
    "Based on the distributional characteristics and practical use cases:\n",
    "\n",
    "- **Primary objective:** Mean Absolute Error (MAE)  \n",
    "\n",
    "- **Secondary metrics:** Log-MAE, Log-RMSE  \n",
    "\n",
    "These design choices reduce sensitivity to heavy-tail noise, prioritize reliable central estimates, and align with the needs of typical users of emissions forecasts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbec3bc6",
   "metadata": {},
   "source": [
    "# 4. Feature Engineering Overview\n",
    "\n",
    "The provided dataset includes **three** broad categories of features:\n",
    "\n",
    "### 1. **Scale** of business\n",
    "\n",
    "- revenue\n",
    "\n",
    "- revenue distribution across NACE Level 2 sectors\n",
    "\n",
    "These proxy for operational size and activity mix.\n",
    "\n",
    "### 2. **Geography**\n",
    "\n",
    "- region\n",
    "\n",
    "- country\n",
    "\n",
    "These relate to energy-grid intensity and regulatory environment.\n",
    "\n",
    "### 3. **Behavioral** and **Sustainability** attributes\n",
    "\n",
    "- environmental/social/governance score\n",
    "\n",
    "- environmental activity adjustments\n",
    "\n",
    "- SDG commitments\n",
    "\n",
    "\n",
    "These reflect environmental maturity and may correlate with emissions performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dfb69f",
   "metadata": {},
   "source": [
    "# 5. Scale of Business\n",
    "\n",
    "### 5.1 Hypothesis 1\n",
    "\n",
    "> Business scale (revenue) is the strongest single driver of emissions.\n",
    "\n",
    "#### Reasoning: \n",
    "Larger operations and higher production volumes lead directly to more emissions.\n",
    "\n",
    "##### Empirical result\n",
    "Spearman correlation between log revenue and total log emissions ≈ **0.42** which is a relatively strong univariate signal.\n",
    "\n",
    "\n",
    "#### Conclusion: \n",
    "Revenue serves as the primary baseline feature.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c5c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis 1: Revenue vs Emissions\n",
    "\n",
    "# 1. Plot Revenue Distribution (Raw vs Log)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.histplot(df['revenue'], kde=True, ax=axes[0])\n",
    "axes[0].set_title('Revenue Distribution (Raw)')\n",
    "\n",
    "sns.histplot(np.log1p(df['revenue']), kde=True, ax=axes[1])\n",
    "axes[1].set_title('Revenue Distribution (Log)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Calculate Log Correlations\n",
    "df['log_revenue'] = np.log1p(df['revenue'])\n",
    "df['log_total_emissions'] = np.log1p(df['target_scope_1'] + df['target_scope_2'])\n",
    "\n",
    "correlation = df['log_revenue'].corr(df['log_total_emissions'])\n",
    "print(f\"Correlation between Log(Revenue) and Log(Total Emissions): {correlation:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcadab4",
   "metadata": {},
   "source": [
    "### 5.2 Hypothesis 2\n",
    "\n",
    "> Combined sectors in companies revenue contribution affects Scope 1 and Scope 2 differently.\n",
    "\n",
    "#### Rationale based on **domain knowledge**\n",
    "\n",
    "Scope 1 related sectors (manufacturing, mining, transportation)\n",
    "→ Companies contributed more revenue in Scope 1 sector\n",
    "→ stronger relationship with Scope 1 emission\n",
    "\n",
    "Scope 2 related sectors (ICT, retail, services)\n",
    "→ Companies contributed more revenue in Scope 2 sector\n",
    "→ stronger relationship with Scope 2 emission\n",
    "\n",
    "#### Procedure\n",
    "\n",
    "1. Define two groups: **scope1-related sectors** and **scope2-related sectors**\n",
    "    - Reasoning behind heauristic categorization consulting with domain expert (=llm) \n",
    "        - For example, Education sector is not belonged to either scope 1 or scope 2\n",
    "\n",
    "2. Categorize each company's revenue distribution into two groups by joining train and revenue_distribution_by_sector tables.\n",
    "\n",
    "3. Compute each company’s scope 1 revenue share and scope 2 revenue share individually.\n",
    "\n",
    "4. Measure correlations & distribution.\n",
    "\n",
    "#### Findings\n",
    "\n",
    "- Sectors could be belonged to both or none of scope 1 & 2. \n",
    "\n",
    "- Scope 1 sector revenue is a subset of scope 2 revenue.\n",
    "\n",
    "- High correlations between scope 1 revenue - scope 1 emission / scope 2 revenue - scope 2 emission.\n",
    "\n",
    "- Company revenue contributions not belonged to neither scopes have relatively low emissions.\n",
    "\n",
    "#### Conclusion: \n",
    "- Use sector-partitioned revenue as target-specific predictive features.\n",
    "\n",
    "#### Features from the above insights\n",
    "- log_total_revenue\n",
    "- log_scope_1_revenue\n",
    "- log_scope_2_revenue\n",
    "- scope1_revenue_present (0/1)\n",
    "- scope2_revenue_present (0/1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa20bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis 2: Sector Analysis\n",
    "\n",
    "# Load sector data and classification\n",
    "sector_df = pd.read_csv('data/revenue_distribution_by_sector.csv')\n",
    "classification_df = pd.read_csv('data/sector_emission_scope_classification.csv')\n",
    "\n",
    "# Merge classification\n",
    "sector_df = pd.merge(sector_df, classification_df, on='nace_level_2_name', how='left')\n",
    "\n",
    "# Fill missing classifications (if any) with False/True default (Scope 2 is universal)\n",
    "sector_df['affects_scope_1'] = sector_df['affects_scope_1'].fillna(False)\n",
    "sector_df['affects_scope_2'] = sector_df['affects_scope_2'].fillna(True)\n",
    "\n",
    "# Create summary table\n",
    "sector_summary = sector_df.groupby(['nace_level_1_code', 'nace_level_1_name', 'nace_level_2_name'])[['affects_scope_1', 'affects_scope_2']].first().reset_index()\n",
    "sector_summary['company_count'] = sector_df.groupby(['nace_level_1_code', 'nace_level_1_name', 'nace_level_2_name']).size().values\n",
    "\n",
    "# Display table\n",
    "from IPython.display import display\n",
    "display(sector_summary)\n",
    "\n",
    "# Calculate Scope 1 / Scope 2 Revenue\n",
    "merged_df = pd.merge(sector_df, df[['entity_id', 'revenue']], on='entity_id', how='inner')\n",
    "\n",
    "# Calculate weighted revenue for each scope\n",
    "# Note: A sector can contribute to BOTH Scope 1 and Scope 2\n",
    "merged_df['scope_1_revenue_part'] = merged_df['revenue'] * merged_df['revenue_pct'] * merged_df['affects_scope_1'].astype(int)\n",
    "merged_df['scope_2_revenue_part'] = merged_df['revenue'] * merged_df['revenue_pct'] * merged_df['affects_scope_2'].astype(int)\n",
    "\n",
    "# Aggregate by entity\n",
    "entity_revenue_split = merged_df.groupby('entity_id')[['scope_1_revenue_part', 'scope_2_revenue_part']].sum().reset_index()\n",
    "entity_revenue_split.rename(columns={'scope_1_revenue_part': 'scope_1_revenue', 'scope_2_revenue_part': 'scope_2_revenue'}, inplace=True)\n",
    "\n",
    "final_df = pd.merge(df, entity_revenue_split, on='entity_id', how='left').fillna(0)\n",
    "\n",
    "# 1. Plot Scope 1 / Scope 2 Revenue Distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "sns.histplot(final_df['scope_1_revenue'], kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Scope 1 Revenue (Raw)')\n",
    "\n",
    "sns.histplot(np.log1p(final_df['scope_1_revenue']), kde=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Scope 1 Revenue (Log)')\n",
    "\n",
    "sns.histplot(final_df['scope_2_revenue'], kde=True, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Scope 2 Revenue (Raw)')\n",
    "\n",
    "sns.histplot(np.log1p(final_df['scope_2_revenue']), kde=True, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Scope 2 Revenue (Log)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Calculate Log Correlations (Filtered for non-zero revenue)\n",
    "final_df['log_scope_1_revenue'] = np.log1p(final_df['scope_1_revenue'])\n",
    "final_df['log_scope_2_revenue'] = np.log1p(final_df['scope_2_revenue'])\n",
    "final_df['log_target_scope_1'] = np.log1p(final_df['target_scope_1'])\n",
    "final_df['log_target_scope_2'] = np.log1p(final_df['target_scope_2'])\n",
    "\n",
    "# Filter for non-zero scope 1 revenue\n",
    "df_s1 = final_df[final_df['scope_1_revenue'] > 0]\n",
    "corr_s1 = df_s1['log_scope_1_revenue'].corr(df_s1['log_target_scope_1'])\n",
    "\n",
    "# Filter for non-zero scope 2 revenue\n",
    "df_s2 = final_df[final_df['scope_2_revenue'] > 0]\n",
    "corr_s2 = df_s2['log_scope_2_revenue'].corr(df_s2['log_target_scope_2'])\n",
    "\n",
    "print(f\"Correlation (Filtered > 0): Log(Scope 1 Revenue) vs Log(Scope 1 Emissions): {corr_s1:.4f}\")\n",
    "print(f\"Correlation (Filtered > 0): Log(Scope 2 Revenue) vs Log(Scope 2 Emissions): {corr_s2:.4f}\")\n",
    "\n",
    "# 3. Report Company Counts\n",
    "has_s1 = (final_df['scope_1_revenue'] > 0).sum()\n",
    "has_s2 = (final_df['scope_2_revenue'] > 0).sum()\n",
    "has_both = ((final_df['scope_1_revenue'] > 0) & (final_df['scope_2_revenue'] > 0)).sum()\n",
    "has_neither = ((final_df['scope_1_revenue'] == 0) & (final_df['scope_2_revenue'] == 0)).sum()\n",
    "\n",
    "print(f\"Companies with Scope 1 Revenue: {has_s1}\")\n",
    "print(f\"Companies with Scope 2 Revenue: {has_s2}\")\n",
    "print(f\"Companies with Both: {has_both}\")\n",
    "print(f\"Companies with NO material sector revenue (Both False): {has_neither}\")\n",
    "\n",
    "# 4. Box Plots for Materiality Groups\n",
    "final_df['materiality_group'] = 'Any Material'\n",
    "final_df.loc[(final_df['scope_1_revenue'] == 0) & (final_df['scope_2_revenue'] == 0), 'materiality_group'] = 'None'\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.boxplot(x='materiality_group', y='log_target_scope_1', data=final_df, ax=axes[0])\n",
    "axes[0].set_title('Log(Scope 1 Emissions) by Materiality')\n",
    "\n",
    "sns.boxplot(x='materiality_group', y='log_target_scope_2', data=final_df, ax=axes[1])\n",
    "axes[1].set_title('Log(Scope 2 Emissions) by Materiality')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fbb002",
   "metadata": {},
   "source": [
    "# TODO: scope 2 = usage * country-wise-factors\n",
    "- scope 1 factor: 국가별 차이 5% ~ 15% 수준\n",
    "- scope 2 factor: 최대 50배, 전력 믹스 구성\n",
    "\n",
    "가설: 사업 규모 (revenue) 가 같으면, 에너지 사용량이 같은데,\n",
    "국가가 다르면 factor가 다르기에 target scope 1/2 value가 차이가 날 것이다\n",
    "\n",
    "scope 1에 대해서 비슷한 규모의 회사들끼리 비교했을때 국가별 차이가 큰지 보기. 기대: 차이 작다\n",
    "scope 2에서 마찬가지. 기대: 차이 크다\n",
    "신뢰수준 볼 수 있을 정도려나\n",
    "기대 결과: scope 2 예측에는 국가 피쳐 의미 있다, 하지만 test set 보면 특정 region만 나와서 의미 별로 없을 것.\n",
    "fairness/bias 문제 조심."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d226eaf",
   "metadata": {},
   "source": [
    "# 6. Geography Analysis\n",
    "Region\n",
    "\n",
    "In the train set, WEU and NAM dominate; other regions have only a handful of records.\n",
    "\n",
    "In the test set, only WEU and NAM appear.\n",
    "\n",
    "Minority regions are grouped as “Others” to reduce noise and instability.\n",
    "\n",
    "Note: potential fairness and bias issues should be revisited in future work.\n",
    "\n",
    "Country\n",
    "\n",
    "Some countries have very few data points.\n",
    "\n",
    "Countries with more samples often show very high variance.\n",
    "\n",
    "Country-level features were therefore removed due to lack of consistent signal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c65c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "def analyze_country_factors(df, scope_col, revenue_col, country_col='country_code', \n",
    "                            n_quantiles=10, use_log_target=False):\n",
    "    \"\"\"\n",
    "    use_log_target=False → 기존 raw target 분석\n",
    "    use_log_target=True  → log1p(target) 기반 안정화된 factor 분석\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Create log target column if needed\n",
    "    if use_log_target:\n",
    "        scope_col_used = f\"log_{scope_col}\"\n",
    "        df[scope_col_used] = np.log1p(df[scope_col])\n",
    "        print(f\"\\n=== Log-Transformed Analysis for {scope_col} vs {revenue_col} ===\")\n",
    "    else:\n",
    "        scope_col_used = scope_col\n",
    "        print(f\"\\n=== Analysis for {scope_col} vs {revenue_col} ===\")\n",
    "    \n",
    "    # Step A: Filter and Quantiles\n",
    "    df_valid = df[df[revenue_col] > 0].copy()\n",
    "    df_valid['log_revenue'] = np.log1p(df_valid[revenue_col])\n",
    "    \n",
    "    # Quantile binning\n",
    "    try:\n",
    "        df_valid['quantile'] = pd.qcut(df_valid['log_revenue'], n_quantiles, labels=False)\n",
    "    except ValueError:\n",
    "        print(\"Warning: Not enough unique revenue values for quantiles. Using rank-based binning.\")\n",
    "        df_valid['quantile'] = pd.qcut(df_valid['log_revenue'].rank(method='first'), n_quantiles, labels=False)\n",
    "    \n",
    "    # Step B-D: Per group analysis\n",
    "    for q in range(n_quantiles):\n",
    "        group_df = df_valid[df_valid['quantile'] == q]\n",
    "        if group_df.empty:\n",
    "            continue\n",
    "        \n",
    "        country_counts = group_df[country_col].value_counts()\n",
    "        if country_counts.empty:\n",
    "            continue\n",
    "        \n",
    "        baseline_country = country_counts.idxmax()\n",
    "        baseline_data = group_df[group_df[country_col] == baseline_country][scope_col_used]\n",
    "        \n",
    "        # Stats\n",
    "        b_min = baseline_data.min()\n",
    "        b_max = baseline_data.max()\n",
    "        b_mean = baseline_data.mean()\n",
    "        b_std = baseline_data.std()\n",
    "        b_median = baseline_data.median()\n",
    "        \n",
    "        print(f\"\\n[Group {q} (Quantile {q+1}/{n_quantiles})]\")\n",
    "        print(f\"- Baseline country: {baseline_country} (n={len(baseline_data)})\")\n",
    "        print(f\"- Baseline stats: min={b_min:.4f}, max={b_max:.4f}, mean={b_mean:.4f}, std={b_std:.4f}, median={b_median:.4f}\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for country in country_counts.index:\n",
    "            if country == baseline_country:\n",
    "                continue\n",
    "            \n",
    "            c_data = group_df[group_df[country_col] == country][scope_col_used]\n",
    "            if len(c_data) < 2:\n",
    "                continue\n",
    "            \n",
    "            c_mean = c_data.mean()\n",
    "            c_std = c_data.std()\n",
    "            \n",
    "            # Ratio changes if using log or raw\n",
    "            if use_log_target:\n",
    "                # In log space: exp(mean difference) gives multiplicative factor\n",
    "                mean_ratio = np.exp(c_mean - b_mean)\n",
    "            else:\n",
    "                mean_ratio = c_mean / b_mean if b_mean != 0 else np.nan\n",
    "            \n",
    "            # Welch test\n",
    "            t_stat, p_val = stats.ttest_ind(c_data, baseline_data, equal_var=False)\n",
    "            is_sig = \"Yes\" if p_val < 0.05 else \"No\"\n",
    "            \n",
    "            results.append({\n",
    "                'Country': country,\n",
    "                'n': len(c_data),\n",
    "                'Mean': c_mean,\n",
    "                'Std': c_std,\n",
    "                'Ratio': mean_ratio,\n",
    "                'p-value': p_val,\n",
    "                'Sig?': is_sig\n",
    "            })\n",
    "        \n",
    "        if results:\n",
    "            print(pd.DataFrame(results).to_string(index=False, float_format=lambda x: \"{:.4f}\".format(x)))\n",
    "        else:\n",
    "            print(\"No other countries with sufficient data for comparison.\")\n",
    "\n",
    "\n",
    "# Run Analysis for Scope 1\n",
    "# Note: Using 'scope_1_revenue' calculated in previous step\n",
    "analyze_country_factors(final_df, 'target_scope_1', 'scope_1_revenue')\n",
    "\n",
    "# Run Analysis for Scope 2\n",
    "# Note: Using 'scope_2_revenue' calculated in previous step\n",
    "analyze_country_factors(final_df, 'target_scope_2', 'scope_2_revenue')\n",
    "\n",
    "# Run Analysis for Log Scope 1\n",
    "analyze_country_factors(final_df, 'target_scope_1', 'scope_1_revenue', use_log_target=True)\n",
    "\n",
    "# Run Analysis for Log Scope 2\n",
    "analyze_country_factors(final_df, 'target_scope_2', 'scope_2_revenue', use_log_target=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62d3258",
   "metadata": {},
   "source": [
    "국가 피쳐는\n",
    "- 단독 feature로는 강한 signal이 없다\n",
    "- scope 1, 2 정의 생각하면 있어야 마땅하다.\n",
    "- 데이터가 적어서 variance가 너무 커서 지금 단계에서는 detect가 안 되었을 뿐이다\n",
    "- 일부는 sig 하다고 나오긴 했다 특히 n이 큰 경우들에 대해서.\n",
    "- 실제 랜덤포레스트/LightGBM 모델에 넣으면 성능 개선에 기여할 가능성 있음 (왜냐하면 도메인 지식 + 약간의 sig 한 결과)\n",
    "\n",
    "Region은\n",
    "- weu, nam 제외하면 나머지 5개 region을 가진 회사가 10개도 안됨\n",
    "- country가 더 세밀한 정보를 제공함\n",
    "\n",
    "=> 결론: 국가 피쳐 쓴다 (categorical). region feature 안쓴다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba35d5bc",
   "metadata": {},
   "source": [
    "# 7. Behavioral Features\n",
    "\n",
    "Not yet fully explored.\n",
    "\n",
    "Potential signals:\n",
    "\n",
    "Negative environmental adjustments indicate beneficial activities.\n",
    "\n",
    "SDG commitments (especially SDG 13 “Climate Action”).\n",
    "\n",
    "ESG scores may reflect process maturity but can be noisy.\n",
    "\n",
    "Further EDA is required to validate their predictive value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a6bb3d",
   "metadata": {},
   "source": [
    "# 7.1 overall_score & e/s/g score\n",
    "- overall = 0.45 * e + 0.3 * s + 0.25 * g => exclude overall score as a feature due to multicolinearity\n",
    "- 1 to 5 scale. closer to 1 the better\n",
    "- data/environmental_activities.csv: e score adjustment. simple addition? (additive adjustment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a12bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load environmental activities\n",
    "env_activities = pd.read_csv('data/environmental_activities.csv')\n",
    "\n",
    "# Aggregate adjustments by entity_id\n",
    "env_adj_agg = env_activities.groupby('entity_id')['env_score_adjustment'].sum().reset_index()\n",
    "\n",
    "# Merge with final_df\n",
    "# We use left merge to keep all companies in final_df, filling missing adjustments with 0\n",
    "analysis_df = final_df.merge(env_adj_agg, on='entity_id', how='left')\n",
    "analysis_df['env_score_adjustment'] = analysis_df['env_score_adjustment'].fillna(0)\n",
    "\n",
    "# Calculate adjusted scores\n",
    "analysis_df['env_adjusted'] = analysis_df['environmental_score'] + analysis_df['env_score_adjustment']\n",
    "analysis_df['overall_adjusted'] = (0.45 * analysis_df['env_adjusted'] + \n",
    "                                   0.3 * analysis_df['social_score'] + \n",
    "                                   0.25 * analysis_df['governance_score'])\n",
    "\n",
    "# Create log targets\n",
    "analysis_df['log_target_scope_1'] = np.log1p(analysis_df['target_scope_1'])\n",
    "analysis_df['log_target_scope_2'] = np.log1p(analysis_df['target_scope_2'])\n",
    "\n",
    "# === Global Correlations (No Quantiles) ===\n",
    "global_targets = [\n",
    "    ('Scope 1 Target', 'target_scope_1'),\n",
    "    ('Log Scope 1 Target', 'log_target_scope_1'),\n",
    "    ('Scope 2 Target', 'target_scope_2'),\n",
    "    ('Log Scope 2 Target', 'log_target_scope_2')\n",
    "]\n",
    "\n",
    "score_cols_map = {\n",
    "    'overall_score': 'overall',\n",
    "    'overall_adjusted': 'overall_adjusted',\n",
    "    'environmental_score': 'env',\n",
    "    'env_adjusted': 'env_adjusted',\n",
    "    'social_score': 'social',\n",
    "    'governance_score': 'governance'\n",
    "}\n",
    "\n",
    "global_results = []\n",
    "for label, t_col in global_targets:\n",
    "    row = {'target': label}\n",
    "    for s_col, s_label in score_cols_map.items():\n",
    "        # Calculate correlation on the full dataset (ignoring NaNs)\n",
    "        row[s_label] = analysis_df[t_col].corr(analysis_df[s_col])\n",
    "    global_results.append(row)\n",
    "\n",
    "global_corr_df = pd.DataFrame(global_results)\n",
    "# Ensure column order\n",
    "global_cols = ['target', 'overall', 'overall_adjusted', 'env', 'env_adjusted', 'social', 'governance']\n",
    "global_corr_df = global_corr_df[global_cols]\n",
    "\n",
    "print(\"=== Global Correlations (All Data) ===\")\n",
    "print(global_corr_df.to_string(index=False, float_format=lambda x: \"{:.4f}\".format(x)))\n",
    "print(\"\\n\")\n",
    "\n",
    "# === Quantile Analysis ===\n",
    "def calculate_quantile_correlations(df, revenue_col, target_col, title):\n",
    "    score_cols = ['overall_score', 'overall_adjusted', 'environmental_score', 'env_adjusted', 'social_score', 'governance_score']\n",
    "    \n",
    "    # Filter for valid revenue\n",
    "    valid_df = df[df[revenue_col] > 0].copy()\n",
    "    \n",
    "    # Create quantiles\n",
    "    n_quantiles = 10\n",
    "    try:\n",
    "        valid_df['quantile'] = pd.qcut(valid_df[revenue_col], n_quantiles, labels=False)\n",
    "    except ValueError:\n",
    "        # Fallback if not enough unique values\n",
    "        valid_df['quantile'] = pd.qcut(valid_df[revenue_col].rank(method='first'), n_quantiles, labels=False)\n",
    "    \n",
    "    # Calculate correlations per group\n",
    "    results = []\n",
    "    for q in range(n_quantiles):\n",
    "        group_data = valid_df[valid_df['quantile'] == q]\n",
    "        if len(group_data) < 2:\n",
    "            continue\n",
    "        \n",
    "        row = {'quantile group': q}\n",
    "        for col in score_cols:\n",
    "            corr = group_data[col].corr(group_data[target_col])\n",
    "            row[col] = corr\n",
    "        results.append(row)\n",
    "    \n",
    "    # Create result dataframe\n",
    "    corr_table = pd.DataFrame(results)\n",
    "    \n",
    "    # Rename columns for display\n",
    "    corr_table = corr_table.rename(columns={\n",
    "        'overall_score': 'overall',\n",
    "        'environmental_score': 'env',\n",
    "        'social_score': 'social',\n",
    "        'governance_score': 'governance'\n",
    "    })\n",
    "    \n",
    "    # Reorder columns\n",
    "    display_cols = ['quantile group', 'overall', 'overall_adjusted', 'env', 'env_adjusted', 'social', 'governance']\n",
    "    corr_table = corr_table[display_cols]\n",
    "    \n",
    "    print(f\"=== {title} ===\")\n",
    "    print(corr_table.to_string(index=False, float_format=lambda x: \"{:.4f}\".format(x)))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# 1. Scope 1 Revenue vs Target Scope 1\n",
    "calculate_quantile_correlations(analysis_df, 'scope_1_revenue', 'target_scope_1', \n",
    "                              'Correlation: Scope 1 Revenue Quantile vs Target Scope 1')\n",
    "\n",
    "# 2. Scope 1 Revenue vs Log Target Scope 1\n",
    "calculate_quantile_correlations(analysis_df, 'scope_1_revenue', 'log_target_scope_1', \n",
    "                              'Correlation: Scope 1 Revenue Quantile vs Log Target Scope 1')\n",
    "\n",
    "# 3. Scope 2 Revenue vs Target Scope 2\n",
    "calculate_quantile_correlations(analysis_df, 'scope_2_revenue', 'target_scope_2', \n",
    "                              'Correlation: Scope 2 Revenue Quantile vs Target Scope 2')\n",
    "\n",
    "# 4. Scope 2 Revenue vs Log Target Scope 2\n",
    "calculate_quantile_correlations(analysis_df, 'scope_2_revenue', 'log_target_scope_2', \n",
    "                              'Correlation: Scope 2 Revenue Quantile vs Log Target Scope 2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a53239",
   "metadata": {},
   "source": [
    "- 산업 규모를 무시하고 보면 correlation 작아보임\n",
    "- 산업 규모(revenue)를 고려하면 그룹마다 correlation 꽤 크게 보이는 경우 있음\n",
    "- 음수/양수 들쭉날쭉 해서 비선형적인 관계겠지만 사용하기에 좋은 피쳐라는 뜻. 어차피 산업 규모(revenue)도 피쳐로 들어가기 때문.\n",
    "- 비선형적 모델 ex) XGBoost 를 쓸때 도움될 것\n",
    "- 그리고 overall vs overall adjusted, env vs env adjusted 비교해봤을때, adjusted score가 consistent하게 더 correlation이 크기에 더 좋은 피쳐다\n",
    "- overall은 e/s/g score의 weighted sum이라 multicolinearity를 유발해서 쓰지 말자\n",
    "\n",
    "결론: esg score feature를 사용하되, env는 adjusted로 써야 한다, overall score는 쓰지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c25d8a",
   "metadata": {},
   "source": [
    "# 7.2 sdg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sdg_analysis_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Load SDG data\n",
    "sdg_df = pd.read_csv('data/sustainable_development_goals.csv')\n",
    "unique_sdgs = sdg_df[['sdg_id', 'sdg_name']].drop_duplicates().sort_values('sdg_id')\n",
    "\n",
    "# Targets to analyze\n",
    "targets = [\n",
    "    ('target_scope_1', 'target scope 1'),\n",
    "    ('log_target_scope_1', 'target scope 1 log'),\n",
    "    ('target_scope_2', 'target scope 2'),\n",
    "    ('log_target_scope_2', 'target scope 2 log')\n",
    "]\n",
    "\n",
    "significant_cases = []\n",
    "\n",
    "for _, row in unique_sdgs.iterrows():\n",
    "    sdg_id = row['sdg_id']\n",
    "    sdg_name = row['sdg_name']\n",
    "    \n",
    "    print(f\"[======={sdg_name} ({sdg_id})=======]\")\n",
    "    \n",
    "    # Identify entities with this SDG\n",
    "    entities_with_sdg = sdg_df[sdg_df['sdg_id'] == sdg_id]['entity_id'].unique()\n",
    "    \n",
    "    has_sdg = analysis_df['entity_id'].isin(entities_with_sdg)\n",
    "    group_exist = analysis_df[has_sdg]\n",
    "    group_non_exist = analysis_df[~has_sdg]\n",
    "    \n",
    "    # Global comparison table\n",
    "    print(\"|target | N(exist) | N(non-exist) | existence mean | non existence mean | p-value | is significant|\")\n",
    "    for col, label in targets:\n",
    "        a = group_exist[col].dropna()\n",
    "        b = group_non_exist[col].dropna()\n",
    "        \n",
    "        n_exist = len(a)\n",
    "        n_non_exist = len(b)\n",
    "        \n",
    "        if n_exist > 1 and n_non_exist > 1:\n",
    "            stat, pval = stats.ttest_ind(a, b, equal_var=False)\n",
    "            mean_exist = a.mean()\n",
    "            mean_non_exist = b.mean()\n",
    "            is_sig = pval < 0.05\n",
    "        else:\n",
    "            mean_exist = np.nan\n",
    "            mean_non_exist = np.nan\n",
    "            pval = np.nan\n",
    "            is_sig = False\n",
    "            \n",
    "        print(f\"|{label}| {n_exist} | {n_non_exist} | {mean_exist:.4f} | {mean_non_exist:.4f} | {pval:.4f} | {is_sig}|\")\n",
    "        \n",
    "        if is_sig:\n",
    "            significant_cases.append({\n",
    "                'type': 'Global',\n",
    "                'sdg': f\"{sdg_name} ({sdg_id})\",\n",
    "                'target': label,\n",
    "                'quantile': 'All',\n",
    "                'p_value': pval,\n",
    "                'mean_diff': mean_exist - mean_non_exist\n",
    "            })\n",
    "            \n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Quantile Analysis Helper\n",
    "    def run_quantile_analysis(revenue_col, target_col, target_label, quantile_label):\n",
    "        print(f\"[{quantile_label}, {target_label} mean comparison]\")\n",
    "        print(\"|quantile group| N(exist) | N(non-exist) | existence mean | non existence mean | p-value | is significant|\")\n",
    "        \n",
    "        temp_df = analysis_df[analysis_df[revenue_col] > 0].copy()\n",
    "        \n",
    "        try:\n",
    "            temp_df['quantile'] = pd.qcut(temp_df[revenue_col], 10, labels=False)\n",
    "        except ValueError:\n",
    "             temp_df['quantile'] = pd.qcut(temp_df[revenue_col].rank(method='first'), 10, labels=False)\n",
    "             \n",
    "        for q in range(10):\n",
    "            q_data = temp_df[temp_df['quantile'] == q]\n",
    "            \n",
    "            has_sdg_q = q_data['entity_id'].isin(entities_with_sdg)\n",
    "            g_exist = q_data[has_sdg_q][target_col].dropna()\n",
    "            g_non_exist = q_data[~has_sdg_q][target_col].dropna()\n",
    "            \n",
    "            n_exist = len(g_exist)\n",
    "            n_non_exist = len(g_non_exist)\n",
    "            \n",
    "            if n_exist > 1 and n_non_exist > 1:\n",
    "                stat, pval = stats.ttest_ind(g_exist, g_non_exist, equal_var=False)\n",
    "                m_exist = g_exist.mean()\n",
    "                m_non_exist = g_non_exist.mean()\n",
    "                is_sig = pval < 0.05\n",
    "            else:\n",
    "                m_exist = np.nan\n",
    "                m_non_exist = np.nan\n",
    "                pval = np.nan\n",
    "                is_sig = False\n",
    "            \n",
    "            print(f\"|{q}| {n_exist} | {n_non_exist} | {m_exist:.4f} | {m_non_exist:.4f} | {pval:.4f} | {is_sig}|\")\n",
    "            \n",
    "            if is_sig:\n",
    "                significant_cases.append({\n",
    "                    'type': 'Quantile',\n",
    "                    'sdg': f\"{sdg_name} ({sdg_id})\",\n",
    "                    'target': target_label,\n",
    "                    'quantile': f\"{quantile_label} (Group {q})\",\n",
    "                    'p_value': pval,\n",
    "                    'mean_diff': m_exist - m_non_exist\n",
    "                })\n",
    "        print(\"\\n\")\n",
    "\n",
    "    run_quantile_analysis('scope_1_revenue', 'target_scope_1', 'target scope 1', 'scope 1 revenue quantile')\n",
    "    run_quantile_analysis('scope_1_revenue', 'log_target_scope_1', 'target scope 1 log', 'scope 1 revenue quantile')\n",
    "    run_quantile_analysis('scope_2_revenue', 'target_scope_2', 'target scope 2', 'scope 2 revenue quantile')\n",
    "    run_quantile_analysis('scope_2_revenue', 'log_target_scope_2', 'target scope 2 log', 'scope 2 revenue quantile')\n",
    "    \n",
    "    print(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "print(\"=== Summary of Significant Cases ===\")\n",
    "if significant_cases:\n",
    "    sig_df = pd.DataFrame(significant_cases)\n",
    "    # Sort by p-value for better readability\n",
    "    sig_df = sig_df.sort_values('p_value')\n",
    "    print(sig_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No significant cases found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e044a74",
   "metadata": {},
   "source": [
    "- SDG는 예측 신호가 아니다\n",
    "- Global에서 유의미해도 규모로 통제하면 사라짐. 즉, “기업 규모가 매우 작은 기업들만 SDG를 가진 경향” 같은 confounding effect일 가능성.\n",
    "- SDG는 자기보고(self-reported)이며 실제 운영과 먼 경우가 많음\n",
    "- 표본이 너무 적음\n",
    "- 따라서 피쳐로 사용하지 않는다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6802dfb1",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "To validate whether the insights from the earlier data analysis actually improve prediction of Scope 1 and Scope 2 emissions, several experiments were conducted. Each experiment combines a preprocessing strategy with a specific model type. The goal is to compare both datasets and modeling approaches in a controlled setting.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Two dataset configurations are compared.\n",
    "\n",
    "### 1. Baseline Dataset\n",
    "This follows the preprocessing pipeline originally provided by the Codeathon organizers. It includes:\n",
    "\n",
    "- One-hot encoding of `region`  \n",
    "- Adding sector-level revenue share using the revenue distribution table  \n",
    "- Adding environmental activity score adjustments  \n",
    "- Adding binary features representing Sustainable Development Goal (SDG) activities  \n",
    "- Applying a variance threshold of 0.05 for feature selection  \n",
    "\n",
    "### 2. Proposed Dataset\n",
    "This dataset uses features identified through exploratory analysis and domain reasoning. The feature set includes:\n",
    "\n",
    "- `log_total_revenue`\n",
    "- `log_scope1_revenue`\n",
    "- `log_scope2_revenue`\n",
    "- `scope1_revenue_present` (0 or 1)\n",
    "- `scope2_revenue_present` (0 or 1)\n",
    "- `env_adjusted_score`\n",
    "- `social_score`\n",
    "- `governance_score`\n",
    "- `country_code`  \n",
    "  - Used directly as a categorical variable for tree-based models  \n",
    "  - One-hot encoded for linear models  \n",
    "\n",
    "The target variables (`target_scope_1`, `target_scope_2`) are log-transformed during training to stabilize variance. Predictions are converted back to raw scale for final evaluation.\n",
    "\n",
    "\n",
    "## Models\n",
    "\n",
    "Three different models are used to evaluate linear trends, robustness to outliers, and nonlinear relationships.\n",
    "\n",
    "### 1. Linear Regression\n",
    "This is the baseline model defined by the organizers. It optimizes MSE and serves as a reference point. Linear regression is sensitive to outliers and cannot capture nonlinearity.\n",
    "\n",
    "### 2. Median Regression\n",
    "Our primary metric is MAE, and secondary metrics are log MAE and log RMSE. Since all emphasize central tendency and robustness, median regression is appropriate. The regularization constant (alpha) is set to 0 to avoid biasing the solution.\n",
    "\n",
    "### 3. CatBoost\n",
    "Given the skewed numerical features and categorical variables such as `country_code`, a tree-based model is well suited. CatBoost handles categorical encoding internally and captures nonlinear effects efficiently, making it a strong candidate for small datasets.\n",
    "\n",
    "\n",
    "## Experiment Settings\n",
    "\n",
    "We evaluate four combinations of dataset and model:\n",
    "\n",
    "1. **Baseline Dataset + Linear Regression**  \n",
    "   This recreates the original baseline setup and serves as a comparison anchor.\n",
    "\n",
    "2. **Baseline Dataset + Median Regression**  \n",
    "   This tests whether replacing the baseline model with a MAE-oriented model improves performance under the same feature configuration.\n",
    "\n",
    "3. **Proposed Dataset + Median Regression**  \n",
    "   This evaluates whether the newly engineered features provide meaningful gains over the baseline features.\n",
    "\n",
    "4. **Proposed Dataset + CatBoost**  \n",
    "   This leverages a tree-based method to capture nonlinear patterns and directly handle categorical variables, aiming for the best overall performance.\n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "The following table summarizes the cross-validated performance of all four experiment settings.  \n",
    "Metrics reported are MAE, log-MAE, and log-RMSE for both Scope 1 and Scope 2 targets.\n",
    "\n",
    "### Final Results Table\n",
    "\n",
    "| Model                                      | Scope 1 MAE | Scope 1 Log MAE | Scope 1 Log RMSE | Scope 2 MAE | Scope 2 Log MAE | Scope 2 Log RMSE |\n",
    "|--------------------------------------------|-------------|-----------------|------------------|-------------|-----------------|------------------|\n",
    "| LinearRegression + Baseline                | 64409.1131  | 2.5848          | 3.5073           | 71708.9193  | 3.1736          | 4.3514           |\n",
    "| MedianRegression + Baseline                | 51830.9031  | 2.0765          | 2.8194           | 52615.5835  | 2.1804          | 3.2238           |\n",
    "| MedianRegression + NewFeatures + LogTarget | 51025.5594  | 1.5479          | 1.9553           | 55099.4216  | 1.8250          | 2.4967           |\n",
    "| CatBoost + NewFeatures + LogTarget         | **50094.6028** | **1.4946**     | **1.8922**       | **52235.8818** | **1.8224**     | **2.5350**       |\n",
    "\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "### 1. Baseline vs Median Regression  \n",
    "- Median regression significantly outperforms linear regression across all metrics.  \n",
    "- This confirms that MAE-oriented modeling and robustness to outliers are important for this task.\n",
    "\n",
    "### 2. Baseline Features vs Proposed Features  \n",
    "- Replacing the baseline features with the proposed analytical features further improves performance.  \n",
    "- Log-transforming the targets stabilizes heavy skewness and reduces both log-MAE and log-RMSE substantially.\n",
    "\n",
    "### 3. CatBoost Performance  \n",
    "- CatBoost delivers the best results for Scope 1 and competitive results for Scope 2.  \n",
    "- Its ability to handle nonlinearity and categorical variables directly gives it an advantage.  \n",
    "- Overall, **CatBoost + Proposed Features + Log Target** is the top-performing setup.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "- Feature engineering based on domain insights leads to consistent improvements.  \n",
    "- Log-target modeling is highly effective for emission data with heavy-tailed distributions.  \n",
    "- CatBoost is the strongest model overall, indicating that nonlinear relationships and categorical structure play an important role in predicting emissions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfeb89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from src.preprocessor import (\n",
    "    CodeathonBaselinePreprocessor,\n",
    "    ProposedFeaturePreprocessor\n",
    ")\n",
    "from src.models import (\n",
    "    LinearRegressionModel,\n",
    "    MedianRegressionModel,\n",
    "    LogTargetCatBoostModel,\n",
    "    LogTargetMedianRegressionModel\n",
    ")\n",
    "from src.trainer import Trainer\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Load Data\n",
    "# -----------------------------------------------------\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "y_scope1 = train_df['target_scope_1']\n",
    "y_scope2 = train_df['target_scope_2']\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Define Experiment Configurations\n",
    "# -----------------------------------------------------\n",
    "experiments = [\n",
    "    {\n",
    "        \"name\": \"LinearRegression + Baseline\",\n",
    "        \"preprocessor\": CodeathonBaselinePreprocessor(),\n",
    "        \"model_class\": LinearRegressionModel,\n",
    "        \"model_params\": {},\n",
    "        \"trainer_params\": {\"n_splits\": 5, \"random_state\": 42},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MedianRegression + Baseline\",\n",
    "        \"preprocessor\": CodeathonBaselinePreprocessor(),\n",
    "        \"model_class\": MedianRegressionModel,\n",
    "        \"model_params\": {},\n",
    "        \"trainer_params\": {\"n_splits\": 5, \"random_state\": 42},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MedianRegression + NewFeatures + LogTarget\",\n",
    "        \"preprocessor\": ProposedFeaturePreprocessor(),\n",
    "        \"model_class\": LogTargetMedianRegressionModel,\n",
    "        \"model_params\": {},\n",
    "        \"trainer_params\": {\"n_splits\": 5, \"random_state\": 42},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"CatBoost + NewFeatures + LogTarget\",\n",
    "        \"preprocessor\": ProposedFeaturePreprocessor(tree=True),\n",
    "        \"model_class\": LogTargetCatBoostModel,\n",
    "        \"model_params\": {\n",
    "            'iterations': 100,\n",
    "            'depth': 6,\n",
    "            'learning_rate': 0.1,\n",
    "            'random_state': 42\n",
    "        },\n",
    "        \"trainer_params\": {\"n_splits\": 5, \"random_state\": 42},\n",
    "    },\n",
    "]\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Run Experiments\n",
    "# -----------------------------------------------------\n",
    "results = []\n",
    "\n",
    "for exp in experiments:\n",
    "    print(f\"\\n=== Running {exp['name']} ===\")\n",
    "\n",
    "    # Preprocess\n",
    "    X = exp[\"preprocessor\"].fit_transform(train_df)\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model_class=exp[\"model_class\"],\n",
    "        model_params=exp[\"model_params\"],\n",
    "        **exp[\"trainer_params\"]\n",
    "    )\n",
    "\n",
    "    # Train scope 1\n",
    "    m1 = trainer.train(X, y_scope1)\n",
    "    # Train scope 2\n",
    "    m2 = trainer.train(X, y_scope2)\n",
    "\n",
    "    # Save results into table row\n",
    "    results.append({\n",
    "        \"model\": exp[\"name\"],\n",
    "        \"scope1_mae\": m1[\"mean_mae\"],\n",
    "        \"scope1_log_mae\": m1[\"mean_log_mae\"],\n",
    "        \"scope1_log_rmse\": m1[\"mean_log_rmse\"],\n",
    "        \"scope2_mae\": m2[\"mean_mae\"],\n",
    "        \"scope2_log_mae\": m2[\"mean_log_mae\"],\n",
    "        \"scope2_log_rmse\": m2[\"mean_log_rmse\"],\n",
    "    })\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Results DataFrame\n",
    "# -----------------------------------------------------\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\\n==================== Final Results Table ====================\")\n",
    "print(\n",
    "    tabulate(\n",
    "        results_df,\n",
    "        headers=\"keys\",\n",
    "        tablefmt=\"github\",\n",
    "        floatfmt=\".4f\",\n",
    "        showindex=False\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d791d1d",
   "metadata": {},
   "source": [
    "# CatBoost Hyperparameter Optimization\n",
    "\n",
    "This section runs a lightweight hyperparameter search to improve the CatBoost model using the proposed feature set and log-transformed targets.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- The proposed feature preprocessor is applied once (`tree=True`) and reused.\n",
    "- A simple **random search** with 20 trials is performed.\n",
    "- Each trial trains two CatBoost models (Scope 1, Scope 2) using 5-fold CV.\n",
    "- Optimization target is the average MAE across both scopes.\n",
    "\n",
    "## Search Space\n",
    "\n",
    "- `iterations`: 200–1000  \n",
    "- `depth`: 4–8  \n",
    "- `learning_rate`: 0.03–0.1  \n",
    "- `l2_leaf_reg`: 1–10  \n",
    "- `random_state`: 42  \n",
    "\n",
    "## Process\n",
    "\n",
    "For each trial:\n",
    "\n",
    "1. Sample hyperparameters  \n",
    "2. Train CatBoost (log-target mode)  \n",
    "3. Compute MAE for Scope 1 and Scope 2  \n",
    "4. Calculate `(MAE1 + MAE2) / 2`  \n",
    "5. Record the result  \n",
    "\n",
    "The best configuration is chosen by sorting on the combined MAE.\n",
    "\n",
    "## Final Step\n",
    "\n",
    "- Retrain CatBoost with the best hyperparameters  \n",
    "- Add results as a new row:  \n",
    "  **\"CatBoost + NewFeatures + LogTarget (Optimized)\"**  \n",
    "- Append to the previous experiment table\n",
    "\n",
    "\n",
    "# Results (Including Optimized CatBoost)\n",
    "\n",
    "### Final Results Table (With Optimization)\n",
    "\n",
    "| Model                                          | Scope 1 MAE | Scope 1 Log MAE | Scope 1 Log RMSE | Scope 2 MAE | Scope 2 Log MAE | Scope 2 Log RMSE |\n",
    "|------------------------------------------------|-------------|-----------------|------------------|-------------|-----------------|------------------|\n",
    "| LinearRegression + Baseline                    | 64409.1131  | 2.5848          | 3.5073           | 71708.9193  | 3.1736          | 4.3514           |\n",
    "| MedianRegression + Baseline                    | 51830.9031  | 2.0765          | 2.8194           | 52615.5835  | 2.1804          | 3.2238           |\n",
    "| MedianRegression + NewFeatures + LogTarget     | 51025.5594  | 1.5479          | 1.9553           | 55099.4216  | 1.8250          | 2.4967           |\n",
    "| CatBoost + NewFeatures + LogTarget             | 50094.6028  | **1.4946**      | 1.8922           | **52235.8818** | 1.8224        | 2.5350           |\n",
    "| **CatBoost + NewFeatures + LogTarget (Optimized)** | **49595.7693** | 1.4977      | **1.8918**       | 53014.5223  | **1.8212**      | **2.5132**       |\n",
    "\n",
    "### Best Values by Metric\n",
    "\n",
    "- **Scope 1 MAE:** Optimized CatBoost  \n",
    "- **Scope 1 Log MAE:** CatBoost (non-optimized)  \n",
    "- **Scope 1 Log RMSE:** Optimized CatBoost  \n",
    "- **Scope 2 MAE:** CatBoost (non-optimized)  \n",
    "- **Scope 2 Log MAE:** Optimized CatBoost  \n",
    "- **Scope 2 Log RMSE:** Optimized CatBoost  \n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "- CatBoost consistently outperforms all linear and median regression baselines.\n",
    "- Hyperparameter optimization **only provides minor improvement**, mainly in Scope 1 metrics.\n",
    "- The non-optimized CatBoost model already performs extremely well across all metrics.\n",
    "- Given the marginal gains and small dataset size, **extensive hyperparameter tuning is unnecessary** for this task.\n",
    "\n",
    "In conclusion, **CatBoost + Proposed Features + LogTarget** is sufficiently strong even without hyperparameter optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef15f335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from src.preprocessor import ProposedFeaturePreprocessor\n",
    "from src.models import LogTargetCatBoostModel\n",
    "from src.trainer import Trainer\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Settings\n",
    "# -----------------------------------------------------\n",
    "N_TRIALS = 20\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "y_scope1 = train_df[\"target_scope_1\"]\n",
    "y_scope2 = train_df[\"target_scope_2\"]\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Preprocess once (tree mode)\n",
    "# -----------------------------------------------------\n",
    "preprocessor = ProposedFeaturePreprocessor(tree=True)\n",
    "X = preprocessor.fit_transform(train_df)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Hyperparameter Search Space\n",
    "# -----------------------------------------------------\n",
    "def sample_params():\n",
    "    return {\n",
    "        \"iterations\": random.choice([200, 300, 500, 800, 1000]),\n",
    "        \"depth\": random.choice([4, 5, 6, 7, 8]),\n",
    "        \"learning_rate\": random.choice([0.03, 0.05, 0.07, 0.1]),\n",
    "        \"l2_leaf_reg\": random.choice([1, 3, 5, 7, 10]),\n",
    "        \"random_state\": RANDOM_STATE\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Optimization Loop\n",
    "# -----------------------------------------------------\n",
    "trial_results = []\n",
    "\n",
    "print(\"\\n=== CatBoost Hyperparameter Optimization ===\")\n",
    "\n",
    "for trial in range(1, N_TRIALS + 1):\n",
    "    params = sample_params()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model_class=LogTargetCatBoostModel,\n",
    "        model_params=params,\n",
    "        n_splits=5,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    m1 = trainer.train(X, y_scope1)\n",
    "    m2 = trainer.train(X, y_scope2)\n",
    "\n",
    "    combined_score = (m1[\"mean_mae\"] + m2[\"mean_mae\"]) / 2\n",
    "\n",
    "    trial_results.append({\n",
    "        \"trial\": trial,\n",
    "        **params,\n",
    "        \"scope1_mae\": m1[\"mean_mae\"],\n",
    "        \"scope2_mae\": m2[\"mean_mae\"],\n",
    "        \"combined\": combined_score\n",
    "    })\n",
    "\n",
    "    print(f\"Trial {trial}/{N_TRIALS}: combined MAE = {combined_score:.3f} | params = {params}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Results data frame\n",
    "# -----------------------------------------------------\n",
    "hpo_results_df = pd.DataFrame(trial_results).sort_values(\"combined\")\n",
    "\n",
    "print(\"\\n\\n==================== Optimization Results ====================\")\n",
    "print(\n",
    "    tabulate(\n",
    "        hpo_results_df,\n",
    "        headers=\"keys\",\n",
    "        tablefmt=\"github\",\n",
    "        floatfmt=\".4f\",\n",
    "        showindex=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Extract best params\n",
    "# -----------------------------------------------------\n",
    "best_row = hpo_results_df.iloc[0].to_dict()\n",
    "best_params = {\n",
    "    key: best_row[key]\n",
    "    for key in [\"iterations\", \"depth\", \"learning_rate\", \"l2_leaf_reg\", \"random_state\"]\n",
    "}\n",
    "\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(best_params)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Retrain CatBoost using best params\n",
    "# =====================================================\n",
    "trainer = Trainer(\n",
    "    model_class=LogTargetCatBoostModel,\n",
    "    model_params=best_params,\n",
    "    n_splits=5,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "m1_best = trainer.train(X, y_scope1)\n",
    "m2_best = trainer.train(X, y_scope2)\n",
    "\n",
    "# Row to append\n",
    "optimized_row = {\n",
    "    \"model\": \"CatBoost + NewFeatures + LogTarget (Optimized)\",\n",
    "    \"scope1_mae\": m1_best[\"mean_mae\"],\n",
    "    \"scope1_log_mae\": m1_best[\"mean_log_mae\"],\n",
    "    \"scope1_log_rmse\": m1_best[\"mean_log_rmse\"],\n",
    "    \"scope2_mae\": m2_best[\"mean_mae\"],\n",
    "    \"scope2_log_mae\": m2_best[\"mean_log_mae\"],\n",
    "    \"scope2_log_rmse\": m2_best[\"mean_log_rmse\"],\n",
    "}\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Append to previous cell's results_df\n",
    "# =====================================================\n",
    "try:\n",
    "    final_results_df = pd.concat(\n",
    "        [results_df, pd.DataFrame([optimized_row])],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    print(\"\\n\\n==================== Final Results Table (With Optimization) ====================\")\n",
    "    print(\n",
    "        tabulate(\n",
    "            final_results_df,\n",
    "            headers=\"keys\",\n",
    "            tablefmt=\"github\",\n",
    "            floatfmt=\".4f\",\n",
    "            showindex=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "except NameError:\n",
    "    print(\"\\n[WARNING] previous cell's results_df not found.\")\n",
    "    print(\"Optimized row:\")\n",
    "    print(optimized_row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a641656",
   "metadata": {},
   "source": [
    "# Final Prediction (submission.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6d2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessor import ProposedFeaturePreprocessor\n",
    "from src.models import LogTargetCatBoostModel\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"\\n--- Training Final Models and Generating Submission ---\")\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = ProposedFeaturePreprocessor(tree=True)\n",
    "\n",
    "# Fit on train and transform\n",
    "X = preprocessor.fit_transform(train_df)\n",
    "X_test = preprocessor.transform(test_df)\n",
    "\n",
    "# Extract targets\n",
    "y_scope1 = train_df['target_scope_1']\n",
    "y_scope2 = train_df['target_scope_2']\n",
    "\n",
    "# Model params (from final.ipynb:L49-L60)\n",
    "model_params = {\n",
    "    'iterations': 100,\n",
    "    'depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Train Scope 1 on full data\n",
    "print(\"Training Scope 1...\")\n",
    "model_s1 = LogTargetCatBoostModel(**model_params)\n",
    "model_s1.fit(X, y_scope1)\n",
    "s1_predictions = model_s1.predict(X_test)\n",
    "\n",
    "# Train Scope 2 on full data\n",
    "print(\"Training Scope 2...\")\n",
    "model_s2 = LogTargetCatBoostModel(**model_params)\n",
    "model_s2.fit(X, y_scope2)\n",
    "s2_predictions = model_s2.predict(X_test)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'entity_id': test_df['entity_id'],\n",
    "    's1_predictions': s1_predictions,\n",
    "    's2_predictions': s2_predictions\n",
    "})\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Submission saved to submission.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e60d9b",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
