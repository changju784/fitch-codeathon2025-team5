{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cad3c776",
   "metadata": {},
   "source": [
    "# 1. Problem Overview\n",
    "\n",
    "#### Purpose\n",
    "\n",
    "This analysis develops predictive models to estimate two company-level operational greenhouse gas emission targets consistent with the Greenhouse Gas Protocol. Accurate, auditable estimates enable downstream ESG reporting, portfolio-level exposure analysis, and risk assessment when disclosures are incomplete.\n",
    "\n",
    "#### Definitions\n",
    "\n",
    "- **Scope 1 — Direct emissions:** Emissions produced by sources owned or directly controlled by the reporting entity (e.g., combustion on site, company vehicle fleets, fugitive process emissions).\n",
    "- **Scope 2 — Indirect emissions (purchased energy):** Emissions attributable to the generation of purchased electricity, heat or steam consumed by the company.\n",
    "\n",
    "#### Why these targets are generally estimated\n",
    "\n",
    "Many firms do not fully disclose emissions. Domain experts explain that financial institutions, corporates, and ESG analysts use modelled Scope 1 and Scope 2 values to fill these gaps for portfolio accounting, regulatory reporting, and basic risk assessment.\n",
    "\n",
    "#### Why both targets are modelled separately\n",
    "\n",
    "- Scope 1 reflects operational intensity and asset ownership; drivers include production scale, fuel use and sector technology.\n",
    "- Scope 2 depends heavily on electricity consumption patterns and grid carbon intensity, which vary by geography and operations.\n",
    "\n",
    "Practical implication: Models should be trained separately for each target while sharing thoughtfully engineered features where appropriate (e.g., revenue, sector exposure, adjusted ESG signals).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b02403",
   "metadata": {},
   "source": [
    "# 2. Target Distribution and ML Objectives\n",
    "\n",
    "## 2.1 Observed distribution\n",
    "\n",
    "- Both Scope 1 and Scope 2 show pronounced right skew and heavy tails. A handful of large industrial firms contribute a disproportionately large share of total emissions.\n",
    "- Applying a log1p transform stabilizes variance and reduces skewness, producing distributions that are more amenable to standard supervised learners.\n",
    "\n",
    "#### Implications for modelling:\n",
    "\n",
    "- Consider log-based metrics for stability.\n",
    "- Maintain separate models for Scope 1 and Scope 2 but use shared pre-processing conventions where appropriate (e.g., clipping and documented transforms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae4d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('data/train.csv')\n",
    "\n",
    "# Plot distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Scope 1\n",
    "sns.histplot(df['target_scope_1'], kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Target Scope 1 Distribution')\n",
    "\n",
    "# Scope 2\n",
    "sns.histplot(df['target_scope_2'], kde=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Target Scope 2 Distribution')\n",
    "\n",
    "# Log Scope 1\n",
    "sns.histplot(np.log1p(df['target_scope_1']), kde=True, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Log(Target Scope 1) Distribution')\n",
    "\n",
    "# Log Scope 2\n",
    "sns.histplot(np.log1p(df['target_scope_2']), kde=True, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Log(Target Scope 2) Distribution')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e7c8db",
   "metadata": {},
   "source": [
    "## 2.2 Why Median-Focused Prediction\n",
    "\n",
    "#### Rationale:\n",
    "\n",
    "- Extreme emission values are frequently driven by asset-level characteristics (e.g., power plants, refineries) that are not observable in the provided dataset; attempting to fit those outliers closely will likely overfit.\n",
    "- Stakeholders typically use aggregated or median estimates at portfolio or sector levels — therefore stability of central estimates is more valuable than perfect tail accuracy.\n",
    "- RMSE is sensitive to a small number of extreme errors; MAE and log-MAE are more robust and align with multiplicative emission behavior.\n",
    "\n",
    "## 2.3 Objective and Evaluation Metrics\n",
    "Based on the distributional characteristics and practical use cases:\n",
    "- **Primary objective:** Mean Absolute Error (MAE)\n",
    "- **Secondary metrics:** Log-MAE, Log-RMSE\n",
    "\n",
    "These design choices reduce sensitivity to heavy-tail noise, prioritize reliable central estimates, and align with the needs of typical users of emissions forecasts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbec3bc6",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering — Overview\n",
    "\n",
    "This section summarizes feature families, the rationale behind each group, and recommended transformations. The goal is to produce robust, interpretable predictors for Scope 1 and Scope 2 while avoiding implicit data leakage. A detailed feature evaluation is provided in the following section.\n",
    "\n",
    "#### Feature families\n",
    "\n",
    "1. Scale (Operational Size)\n",
    "- Motivation: Revenue is a proxy for production scale and energy use.\n",
    "- Candidate features: `revenue`, `log_revenue`, sector-partitioned absolute revenues, NACE revenue shares.\n",
    "- Strategy: Provide both raw and log-transformed variants; derive scope-specific absolute revenues.\n",
    "\n",
    "2. Geography\n",
    "- Motivation: Under the Greenhouse Gas Protocol, both Scope 1 and Scope 2 vary significantly by country due to fuel mixes, industrial structure, and grid emission factors.\n",
    "- Candidate features: `country_code` (categorical), region groups (collapse rare categories), country emission-factor lookups where available.\n",
    "- Strategy: Use stable encodings (group rare categories, regularized target encoding for high-cardinality country codes).\n",
    "\n",
    "3. Behavioural & Sustainability Signals\n",
    "- Motivation: ESG scores, environmental activity adjustments and SDG commitments proxy for mitigation efforts and process maturity.\n",
    "- Candidate features: adjusted `environmental_score`, `social_score`, `governance_score`, aggregated `env_score_adjustment`, SDG presence indicators.\n",
    "- Strategy: Use adjusted environmental score rather than composite overall score to avoid multicollinearity; clip and standardize consistently across splits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5dfb69f",
   "metadata": {},
   "source": [
    "# 4. Scale of Business\n",
    "\n",
    "## 4.1 Hypothesis 1\n",
    "\n",
    "**Hypothesis:** Business scale, proxied by revenue, is one of the strongest predictors of operational emissions. Applying a log transformation to revenue significantly improves predictability.\n",
    "\n",
    "### Evidence\n",
    "\n",
    "- Both revenue and emissions are heavily right-skewed; applying log transformations produces more stable, bell-shaped distributions.\n",
    "- Correlation analysis indicates a strong positive relationship between `log(revenue)` and `log(total emissions)` (correlation = 0.4291).\n",
    "\n",
    "### Interpretation and Practical Takeaways\n",
    "\n",
    "- Include `log_revenue` as a baseline feature; it stabilizes variance and improves performance for linear models.\n",
    "- Scale is informative but not sufficient. Combining size features with sector exposure and geography provides better discrimination across firms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825c5c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis 1: Revenue vs Emissions\n",
    "\n",
    "# 1. Plot Revenue Distribution (Raw vs Log)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "sns.histplot(df['revenue'], kde=True, ax=axes[0])\n",
    "axes[0].set_title('Revenue Distribution (Raw)')\n",
    "\n",
    "sns.histplot(np.log1p(df['revenue']), kde=True, ax=axes[1])\n",
    "axes[1].set_title('Revenue Distribution (Log)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Calculate Log Correlations\n",
    "df['log_revenue'] = np.log1p(df['revenue'])\n",
    "df['log_total_emissions'] = np.log1p(df['target_scope_1'] + df['target_scope_2'])\n",
    "\n",
    "correlation = df['log_revenue'].corr(df['log_total_emissions'])\n",
    "print(f\"Correlation between Log(Revenue) and Log(Total Emissions): {correlation:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcadab4",
   "metadata": {},
   "source": [
    "## 4.2 Hypothesis 2\n",
    "\n",
    "**Hypothesis:** Sector-partitioned revenues provide additional, target-specific predictive power for Scope 1 and Scope 2.\n",
    "\n",
    "### Procedure Summary\n",
    "\n",
    "1. Map NACE sectors to scope-exposure categories (Scope 1, Scope 2, both, or neither) using conservative domain rules informed by expert guidance.\n",
    "2. Compute absolute revenue per sector by multiplying total revenue by each sector’s revenue share.\n",
    "3. Aggregate sector-level revenues into `scope_1_revenue` and `scope_2_revenue` buckets.\n",
    "4. Generate log-transformed features and binary exposure flags:  \n",
    "   - `log_scope_1_revenue`, `log_scope_2_revenue`  \n",
    "   - `scope1_revenue_present`, `scope2_revenue_present`\n",
    "\n",
    "### Evidence and Rationale\n",
    "\n",
    "- Sectors with direct process emissions (manufacturing, mining, heavy industry) show stronger associations with Scope 1.\n",
    "- Service, ICT, and office-based sectors correlate more with electricity use and thus Scope 2.\n",
    "- Correlation analyses within non-zero subsets indicate that scope-partitioned revenues carry incremental predictive value beyond total revenue.\n",
    "- Candle plots confirm that firms with no material sector revenue exhibit distinctively low emissions profiles.\n",
    "\n",
    "### Interpretation and Practical Takeaways\n",
    "\n",
    "- Include `log_scope_1_revenue` and `log_scope_2_revenue` as core features.\n",
    "- Add binary materiality flags (`scope1_revenue_present`, `scope2_revenue_present`) to capture exposure even when revenue magnitudes are small.\n",
    "- All subsequent analyses condition on company size by controlling for revenue before examining additional relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa20bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis 2: Sector Analysis\n",
    "\n",
    "# Load sector data and classification\n",
    "sector_df = pd.read_csv('data/revenue_distribution_by_sector.csv')\n",
    "classification_df = pd.read_csv('data/sector_emission_scope_classification.csv')\n",
    "\n",
    "# Merge classification\n",
    "sector_df = pd.merge(sector_df, classification_df, on='nace_level_2_name', how='left')\n",
    "\n",
    "# Fill missing classifications (if any) with False/True default (Scope 2 is universal)\n",
    "sector_df['affects_scope_1'] = sector_df['affects_scope_1'].fillna(False)\n",
    "sector_df['affects_scope_2'] = sector_df['affects_scope_2'].fillna(True)\n",
    "\n",
    "# Create summary table\n",
    "sector_summary = sector_df.groupby(['nace_level_1_code', 'nace_level_1_name', 'nace_level_2_name'])[['affects_scope_1', 'affects_scope_2']].first().reset_index()\n",
    "sector_summary['company_count'] = sector_df.groupby(['nace_level_1_code', 'nace_level_1_name', 'nace_level_2_name']).size().values\n",
    "\n",
    "# Display table\n",
    "from IPython.display import display\n",
    "display(sector_summary)\n",
    "\n",
    "# Calculate Scope 1 / Scope 2 Revenue\n",
    "merged_df = pd.merge(sector_df, df[['entity_id', 'revenue']], on='entity_id', how='inner')\n",
    "\n",
    "# Calculate weighted revenue for each scope\n",
    "# Note: A sector can contribute to BOTH Scope 1 and Scope 2\n",
    "merged_df['scope_1_revenue_part'] = merged_df['revenue'] * merged_df['revenue_pct'] * merged_df['affects_scope_1'].astype(int)\n",
    "merged_df['scope_2_revenue_part'] = merged_df['revenue'] * merged_df['revenue_pct'] * merged_df['affects_scope_2'].astype(int)\n",
    "\n",
    "# Aggregate by entity\n",
    "entity_revenue_split = merged_df.groupby('entity_id')[['scope_1_revenue_part', 'scope_2_revenue_part']].sum().reset_index()\n",
    "entity_revenue_split.rename(columns={'scope_1_revenue_part': 'scope_1_revenue', 'scope_2_revenue_part': 'scope_2_revenue'}, inplace=True)\n",
    "\n",
    "final_df = pd.merge(df, entity_revenue_split, on='entity_id', how='left').fillna(0)\n",
    "\n",
    "# 1. Plot Scope 1 / Scope 2 Revenue Distributions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "sns.histplot(final_df['scope_1_revenue'], kde=True, ax=axes[0, 0])\n",
    "axes[0, 0].set_title('Scope 1 Revenue (Raw)')\n",
    "\n",
    "sns.histplot(np.log1p(final_df['scope_1_revenue']), kde=True, ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Scope 1 Revenue (Log)')\n",
    "\n",
    "sns.histplot(final_df['scope_2_revenue'], kde=True, ax=axes[1, 0])\n",
    "axes[1, 0].set_title('Scope 2 Revenue (Raw)')\n",
    "\n",
    "sns.histplot(np.log1p(final_df['scope_2_revenue']), kde=True, ax=axes[1, 1])\n",
    "axes[1, 1].set_title('Scope 2 Revenue (Log)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 2. Calculate Log Correlations (Filtered for non-zero revenue)\n",
    "final_df['log_scope_1_revenue'] = np.log1p(final_df['scope_1_revenue'])\n",
    "final_df['log_scope_2_revenue'] = np.log1p(final_df['scope_2_revenue'])\n",
    "final_df['log_target_scope_1'] = np.log1p(final_df['target_scope_1'])\n",
    "final_df['log_target_scope_2'] = np.log1p(final_df['target_scope_2'])\n",
    "\n",
    "# Filter for non-zero scope 1 revenue\n",
    "df_s1 = final_df[final_df['scope_1_revenue'] > 0]\n",
    "corr_s1 = df_s1['log_scope_1_revenue'].corr(df_s1['log_target_scope_1'])\n",
    "\n",
    "# Filter for non-zero scope 2 revenue\n",
    "df_s2 = final_df[final_df['scope_2_revenue'] > 0]\n",
    "corr_s2 = df_s2['log_scope_2_revenue'].corr(df_s2['log_target_scope_2'])\n",
    "\n",
    "print(f\"Correlation (Filtered > 0): Log(Scope 1 Revenue) vs Log(Scope 1 Emissions): {corr_s1:.4f}\")\n",
    "print(f\"Correlation (Filtered > 0): Log(Scope 2 Revenue) vs Log(Scope 2 Emissions): {corr_s2:.4f}\")\n",
    "\n",
    "# 3. Report Company Counts\n",
    "has_s1 = (final_df['scope_1_revenue'] > 0).sum()\n",
    "has_s2 = (final_df['scope_2_revenue'] > 0).sum()\n",
    "has_both = ((final_df['scope_1_revenue'] > 0) & (final_df['scope_2_revenue'] > 0)).sum()\n",
    "has_neither = ((final_df['scope_1_revenue'] == 0) & (final_df['scope_2_revenue'] == 0)).sum()\n",
    "\n",
    "print(f\"Companies with Scope 1 Revenue: {has_s1}\")\n",
    "print(f\"Companies with Scope 2 Revenue: {has_s2}\")\n",
    "print(f\"Companies with Both: {has_both}\")\n",
    "print(f\"Companies with NO material sector revenue (Both False): {has_neither}\")\n",
    "\n",
    "# 4. Box Plots for Materiality Groups\n",
    "final_df['materiality_group'] = 'Any Material'\n",
    "final_df.loc[(final_df['scope_1_revenue'] == 0) & (final_df['scope_2_revenue'] == 0), 'materiality_group'] = 'None'\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "sns.boxplot(x='materiality_group', y='log_target_scope_1', data=final_df, ax=axes[0])\n",
    "axes[0].set_title('Log(Scope 1 Emissions) by Materiality')\n",
    "\n",
    "sns.boxplot(x='materiality_group', y='log_target_scope_2', data=final_df, ax=axes[1])\n",
    "axes[1].set_title('Log(Scope 2 Emissions) by Materiality')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d226eaf",
   "metadata": {},
   "source": [
    "## 5. Geography\n",
    "\n",
    "### 5.1 Hypothesis 3\n",
    "\n",
    "**Hypothesis:** For companies with comparable scale and sector exposure, country-level factors explain residual differences in emissions. This effect is expected to be stronger for Scope 2 than for Scope 1.\n",
    "\n",
    "### Motivation\n",
    "\n",
    "Domain knowledge suggests that, under the Greenhouse Gas Protocol, both Scope 1 and Scope 2 vary meaningfully by country due to differences in fuel mixes, industrial structure, and grid emission factors. Geography therefore serves as a reasonable proxy for country-level determinants. We sought to validate this using the available data, even though high-quality country emission-factor tables were not included in the provided dataset.\n",
    "\n",
    "### Analytical Approach\n",
    "\n",
    "- Condition on company size by stratifying firms into revenue quantiles and comparing country distributions within each bin.  \n",
    "- Apply Welch’s t-tests to evaluate mean differences and report multiplicative ratios in log space when informative.\n",
    "\n",
    "### Expected Outcomes\n",
    "\n",
    "- **Scope 1:** Only modest country-level differences, likely on the order of single-digit percent effects.  \n",
    "- **Scope 2:** Larger variations driven by grid mix, potentially resulting in substantial multiplicative differences.\n",
    "\n",
    "### Findings and Conclusions\n",
    "\n",
    "**Country features**\n",
    "- As a standalone feature, country shows limited signal due to high variance and small sample sizes in many categories.\n",
    "- However, based on the definitions of Scope 1 and Scope 2, country-level factors should logically matter.\n",
    "- The lack of strong detection is likely due to insufficient data rather than absence of effect; some countries with larger sample sizes do show statistically significant differences.\n",
    "- Given the domain rationale and partial empirical support, country information is likely to improve performance in tree-based models (e.g., CatBoost).\n",
    "\n",
    "**Region features**\n",
    "- Aside from WEU and NAM, all other regions contain fewer than ten companies.\n",
    "- Regions provide much coarser information than country codes and show weaker differentiation.\n",
    "\n",
    "**Final Decision**\n",
    "- **Use country as a categorical feature.**  \n",
    "- **Exclude region features** due to sparsity and lower informational value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c65c09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hypothesis 3: Country Analysis\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "def analyze_country_factors(df, scope_col, revenue_col, country_col='country_code', \n",
    "                            n_quantiles=10, use_log_target=False):\n",
    "    \"\"\"\n",
    "    use_log_target=False → 기존 raw target 분석\n",
    "    use_log_target=True  → log1p(target) 기반 안정화된 factor 분석\n",
    "    \"\"\"\n",
    "    \n",
    "    df = df.copy()\n",
    "    \n",
    "    # Create log target column if needed\n",
    "    if use_log_target:\n",
    "        scope_col_used = f\"log_{scope_col}\"\n",
    "        df[scope_col_used] = np.log1p(df[scope_col])\n",
    "        print(f\"\\n=== Log-Transformed Analysis for {scope_col} vs {revenue_col} ===\")\n",
    "    else:\n",
    "        scope_col_used = scope_col\n",
    "        print(f\"\\n=== Analysis for {scope_col} vs {revenue_col} ===\")\n",
    "    \n",
    "    # Step A: Filter and Quantiles\n",
    "    df_valid = df[df[revenue_col] > 0].copy()\n",
    "    df_valid['log_revenue'] = np.log1p(df_valid[revenue_col])\n",
    "    \n",
    "    # Quantile binning\n",
    "    try:\n",
    "        df_valid['quantile'] = pd.qcut(df_valid['log_revenue'], n_quantiles, labels=False)\n",
    "    except ValueError:\n",
    "        print(\"Warning: Not enough unique revenue values for quantiles. Using rank-based binning.\")\n",
    "        df_valid['quantile'] = pd.qcut(df_valid['log_revenue'].rank(method='first'), n_quantiles, labels=False)\n",
    "    \n",
    "    # Step B-D: Per group analysis\n",
    "    for q in range(n_quantiles):\n",
    "        group_df = df_valid[df_valid['quantile'] == q]\n",
    "        if group_df.empty:\n",
    "            continue\n",
    "        \n",
    "        country_counts = group_df[country_col].value_counts()\n",
    "        if country_counts.empty:\n",
    "            continue\n",
    "        \n",
    "        baseline_country = country_counts.idxmax()\n",
    "        baseline_data = group_df[group_df[country_col] == baseline_country][scope_col_used]\n",
    "        \n",
    "        # Stats\n",
    "        b_min = baseline_data.min()\n",
    "        b_max = baseline_data.max()\n",
    "        b_mean = baseline_data.mean()\n",
    "        b_std = baseline_data.std()\n",
    "        b_median = baseline_data.median()\n",
    "        \n",
    "        print(f\"\\n[Group {q} (Quantile {q+1}/{n_quantiles})]\")\n",
    "        print(f\"- Baseline country: {baseline_country} (n={len(baseline_data)})\")\n",
    "        print(f\"- Baseline stats: min={b_min:.4f}, max={b_max:.4f}, mean={b_mean:.4f}, std={b_std:.4f}, median={b_median:.4f}\")\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for country in country_counts.index:\n",
    "            if country == baseline_country:\n",
    "                continue\n",
    "            \n",
    "            c_data = group_df[group_df[country_col] == country][scope_col_used]\n",
    "            if len(c_data) < 2:\n",
    "                continue\n",
    "            \n",
    "            c_mean = c_data.mean()\n",
    "            c_std = c_data.std()\n",
    "            \n",
    "            # Ratio changes if using log or raw\n",
    "            if use_log_target:\n",
    "                # In log space: exp(mean difference) gives multiplicative factor\n",
    "                mean_ratio = np.exp(c_mean - b_mean)\n",
    "            else:\n",
    "                mean_ratio = c_mean / b_mean if b_mean != 0 else np.nan\n",
    "            \n",
    "            # Welch test\n",
    "            t_stat, p_val = stats.ttest_ind(c_data, baseline_data, equal_var=False)\n",
    "            is_sig = \"Yes\" if p_val < 0.05 else \"No\"\n",
    "            \n",
    "            results.append({\n",
    "                'Country': country,\n",
    "                'n': len(c_data),\n",
    "                'Mean': c_mean,\n",
    "                'Std': c_std,\n",
    "                'Ratio': mean_ratio,\n",
    "                'p-value': p_val,\n",
    "                'Sig?': is_sig\n",
    "            })\n",
    "        \n",
    "        if results:\n",
    "            print(pd.DataFrame(results).to_string(index=False, float_format=lambda x: \"{:.4f}\".format(x)))\n",
    "        else:\n",
    "            print(\"No other countries with sufficient data for comparison.\")\n",
    "\n",
    "\n",
    "# Run Analysis for Scope 1\n",
    "# Note: Using 'scope_1_revenue' calculated in previous step\n",
    "analyze_country_factors(final_df, 'target_scope_1', 'scope_1_revenue')\n",
    "\n",
    "# Run Analysis for Scope 2\n",
    "# Note: Using 'scope_2_revenue' calculated in previous step\n",
    "analyze_country_factors(final_df, 'target_scope_2', 'scope_2_revenue')\n",
    "\n",
    "# Run Analysis for Log Scope 1\n",
    "analyze_country_factors(final_df, 'target_scope_1', 'scope_1_revenue', use_log_target=True)\n",
    "\n",
    "# Run Analysis for Log Scope 2\n",
    "analyze_country_factors(final_df, 'target_scope_2', 'scope_2_revenue', use_log_target=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a6bb3d",
   "metadata": {},
   "source": [
    "# 6. Behavioral Features\n",
    "\n",
    "### Overall Score and E/S/G Scores\n",
    "\n",
    "- `overall_score` is a weighted sum of the component scores (`0.45 * E + 0.30 * S + 0.25 * G`).  \n",
    "  → Exclude due to multicollinearity with the underlying E/S/G features.  \n",
    "- Scores are reported on a 1–5 scale, where lower values indicate better performance.  \n",
    "- `data/environmental_activities.csv` provides additive adjustments to the environmental score (`env_score_adjustment`).\n",
    "\n",
    "### Analytical Approach\n",
    "\n",
    "- Compute correlations between emissions and the following features:  \n",
    "  `overall_score`, `environmental_score`, `social_score`, `governance_score`, `env_adjusted_score`, `overall_adjusted_score`.\n",
    "- Condition on company size by stratifying firms into revenue quantiles and re-computing correlations within each bin.\n",
    "\n",
    "### Technical Conclusions — ESG and Adjusted Scores\n",
    "\n",
    "- When ignoring scale, correlations between emissions and ESG scores appear small.\n",
    "- After conditioning on revenue, several quantile groups show meaningful positive or negative correlations, indicating useful signal once scale effects are removed.\n",
    "- The mixed positive/negative patterns across bins suggest a nonlinear relationship, which is well-suited for nonlinear models such as XGBoost.\n",
    "- Adjusted scores (`env_adjusted_score`, `overall_adjusted_score`) consistently exhibit stronger correlations than their unadjusted counterparts.\n",
    "- `overall_score` should not be used because it is a deterministic weighted sum of E/S/G components and introduces multicollinearity.\n",
    "\n",
    "### Final Decision\n",
    "\n",
    "- Use E/S/G component scores as features.  \n",
    "- Prefer `env_adjusted_score` over the raw environmental score.  \n",
    "- Do **not** use `overall_score` (or its adjusted version) due to multicollinearity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a12bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load environmental activities\n",
    "env_activities = pd.read_csv('data/environmental_activities.csv')\n",
    "\n",
    "# Aggregate adjustments by entity_id\n",
    "env_adj_agg = env_activities.groupby('entity_id')['env_score_adjustment'].sum().reset_index()\n",
    "\n",
    "# Merge with final_df\n",
    "# We use left merge to keep all companies in final_df, filling missing adjustments with 0\n",
    "analysis_df = final_df.merge(env_adj_agg, on='entity_id', how='left')\n",
    "analysis_df['env_score_adjustment'] = analysis_df['env_score_adjustment'].fillna(0)\n",
    "\n",
    "# Calculate adjusted scores\n",
    "analysis_df['env_adjusted'] = analysis_df['environmental_score'] + analysis_df['env_score_adjustment']\n",
    "analysis_df['overall_adjusted'] = (0.45 * analysis_df['env_adjusted'] + \n",
    "                                   0.3 * analysis_df['social_score'] + \n",
    "                                   0.25 * analysis_df['governance_score'])\n",
    "\n",
    "# Create log targets\n",
    "analysis_df['log_target_scope_1'] = np.log1p(analysis_df['target_scope_1'])\n",
    "analysis_df['log_target_scope_2'] = np.log1p(analysis_df['target_scope_2'])\n",
    "\n",
    "# === Global Correlations (No Quantiles) ===\n",
    "global_targets = [\n",
    "    ('Scope 1 Target', 'target_scope_1'),\n",
    "    ('Log Scope 1 Target', 'log_target_scope_1'),\n",
    "    ('Scope 2 Target', 'target_scope_2'),\n",
    "    ('Log Scope 2 Target', 'log_target_scope_2')\n",
    "]\n",
    "\n",
    "score_cols_map = {\n",
    "    'overall_score': 'overall',\n",
    "    'overall_adjusted': 'overall_adjusted',\n",
    "    'environmental_score': 'env',\n",
    "    'env_adjusted': 'env_adjusted',\n",
    "    'social_score': 'social',\n",
    "    'governance_score': 'governance'\n",
    "}\n",
    "\n",
    "global_results = []\n",
    "for label, t_col in global_targets:\n",
    "    row = {'target': label}\n",
    "    for s_col, s_label in score_cols_map.items():\n",
    "        # Calculate correlation on the full dataset (ignoring NaNs)\n",
    "        row[s_label] = analysis_df[t_col].corr(analysis_df[s_col])\n",
    "    global_results.append(row)\n",
    "\n",
    "global_corr_df = pd.DataFrame(global_results)\n",
    "# Ensure column order\n",
    "global_cols = ['target', 'overall', 'overall_adjusted', 'env', 'env_adjusted', 'social', 'governance']\n",
    "global_corr_df = global_corr_df[global_cols]\n",
    "\n",
    "print(\"=== Global Correlations (All Data) ===\")\n",
    "print(global_corr_df.to_string(index=False, float_format=lambda x: \"{:.4f}\".format(x)))\n",
    "print(\"\\n\")\n",
    "\n",
    "# === Quantile Analysis ===\n",
    "def calculate_quantile_correlations(df, revenue_col, target_col, title):\n",
    "    score_cols = ['overall_score', 'overall_adjusted', 'environmental_score', 'env_adjusted', 'social_score', 'governance_score']\n",
    "    \n",
    "    # Filter for valid revenue\n",
    "    valid_df = df[df[revenue_col] > 0].copy()\n",
    "    \n",
    "    # Create quantiles\n",
    "    n_quantiles = 10\n",
    "    try:\n",
    "        valid_df['quantile'] = pd.qcut(valid_df[revenue_col], n_quantiles, labels=False)\n",
    "    except ValueError:\n",
    "        # Fallback if not enough unique values\n",
    "        valid_df['quantile'] = pd.qcut(valid_df[revenue_col].rank(method='first'), n_quantiles, labels=False)\n",
    "    \n",
    "    # Calculate correlations per group\n",
    "    results = []\n",
    "    for q in range(n_quantiles):\n",
    "        group_data = valid_df[valid_df['quantile'] == q]\n",
    "        if len(group_data) < 2:\n",
    "            continue\n",
    "        \n",
    "        row = {'quantile group': q}\n",
    "        for col in score_cols:\n",
    "            corr = group_data[col].corr(group_data[target_col])\n",
    "            row[col] = corr\n",
    "        results.append(row)\n",
    "    \n",
    "    # Create result dataframe\n",
    "    corr_table = pd.DataFrame(results)\n",
    "    \n",
    "    # Rename columns for display\n",
    "    corr_table = corr_table.rename(columns={\n",
    "        'overall_score': 'overall',\n",
    "        'environmental_score': 'env',\n",
    "        'social_score': 'social',\n",
    "        'governance_score': 'governance'\n",
    "    })\n",
    "    \n",
    "    # Reorder columns\n",
    "    display_cols = ['quantile group', 'overall', 'overall_adjusted', 'env', 'env_adjusted', 'social', 'governance']\n",
    "    corr_table = corr_table[display_cols]\n",
    "    \n",
    "    print(f\"=== {title} ===\")\n",
    "    print(corr_table.to_string(index=False, float_format=lambda x: \"{:.4f}\".format(x)))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# 1. Scope 1 Revenue vs Target Scope 1\n",
    "calculate_quantile_correlations(analysis_df, 'scope_1_revenue', 'target_scope_1', \n",
    "                              'Correlation: Scope 1 Revenue Quantile vs Target Scope 1')\n",
    "\n",
    "# 2. Scope 1 Revenue vs Log Target Scope 1\n",
    "calculate_quantile_correlations(analysis_df, 'scope_1_revenue', 'log_target_scope_1', \n",
    "                              'Correlation: Scope 1 Revenue Quantile vs Log Target Scope 1')\n",
    "\n",
    "# 3. Scope 2 Revenue vs Target Scope 2\n",
    "calculate_quantile_correlations(analysis_df, 'scope_2_revenue', 'target_scope_2', \n",
    "                              'Correlation: Scope 2 Revenue Quantile vs Target Scope 2')\n",
    "\n",
    "# 4. Scope 2 Revenue vs Log Target Scope 2\n",
    "calculate_quantile_correlations(analysis_df, 'scope_2_revenue', 'log_target_scope_2', \n",
    "                              'Correlation: Scope 2 Revenue Quantile vs Log Target Scope 2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c25d8a",
   "metadata": {},
   "source": [
    "## 6.2 Sustainable Development Goals (SDGs)\n",
    "\n",
    "### What are SDGs?\n",
    "- SDGs reflect high-level sustainability commitments, such as:\n",
    "  - SDG 7: Affordable and Clean Energy  \n",
    "  - SDG 9: Industry, Innovation, and Infrastructure  \n",
    "  - SDG 12: Responsible Consumption and Production  \n",
    "  - SDG 13: Climate Action  \n",
    "\n",
    "### Analytical Approach\n",
    "\n",
    "- For each SDG, compare emissions between companies with and without the SDG commitment using Welch’s t-tests.\n",
    "- Condition on company size by stratifying firms into revenue quantiles and repeat the tests within each bin.\n",
    "\n",
    "### Technical Conclusions: SDGs do not provide predictive signal for emissions.\n",
    "- Sample sizes are very small; no SDG pair shows consistent statistical significance.\n",
    "- Effects visible at the global level disappear once controlling for revenue, indicating confounding (e.g., only small firms disproportionately reporting SDG commitments).\n",
    "- SDGs are self-reported and often loosely connected to operational practices.\n",
    "- The dataset is too sparse to avoid overfitting if encoded as features.\n",
    "\n",
    "### Final Decision\n",
    "- Do **not** include SDG-related features in the predictive model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sdg_analysis_code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Load SDG data\n",
    "sdg_df = pd.read_csv('data/sustainable_development_goals.csv')\n",
    "unique_sdgs = sdg_df[['sdg_id', 'sdg_name']].drop_duplicates().sort_values('sdg_id')\n",
    "\n",
    "# Targets to analyze\n",
    "targets = [\n",
    "    ('target_scope_1', 'target scope 1'),\n",
    "    ('log_target_scope_1', 'target scope 1 log'),\n",
    "    ('target_scope_2', 'target scope 2'),\n",
    "    ('log_target_scope_2', 'target scope 2 log')\n",
    "]\n",
    "\n",
    "significant_cases = []\n",
    "\n",
    "for _, row in unique_sdgs.iterrows():\n",
    "    sdg_id = row['sdg_id']\n",
    "    sdg_name = row['sdg_name']\n",
    "    \n",
    "    print(f\"[======={sdg_name} ({sdg_id})=======]\")\n",
    "    \n",
    "    # Identify entities with this SDG\n",
    "    entities_with_sdg = sdg_df[sdg_df['sdg_id'] == sdg_id]['entity_id'].unique()\n",
    "    \n",
    "    has_sdg = analysis_df['entity_id'].isin(entities_with_sdg)\n",
    "    group_exist = analysis_df[has_sdg]\n",
    "    group_non_exist = analysis_df[~has_sdg]\n",
    "    \n",
    "    # Global comparison table\n",
    "    print(\"|target | N(exist) | N(non-exist) | existence mean | non existence mean | p-value | is significant|\")\n",
    "    for col, label in targets:\n",
    "        a = group_exist[col].dropna()\n",
    "        b = group_non_exist[col].dropna()\n",
    "        \n",
    "        n_exist = len(a)\n",
    "        n_non_exist = len(b)\n",
    "        \n",
    "        if n_exist > 1 and n_non_exist > 1:\n",
    "            stat, pval = stats.ttest_ind(a, b, equal_var=False)\n",
    "            mean_exist = a.mean()\n",
    "            mean_non_exist = b.mean()\n",
    "            is_sig = pval < 0.05\n",
    "        else:\n",
    "            mean_exist = np.nan\n",
    "            mean_non_exist = np.nan\n",
    "            pval = np.nan\n",
    "            is_sig = False\n",
    "            \n",
    "        print(f\"|{label}| {n_exist} | {n_non_exist} | {mean_exist:.4f} | {mean_non_exist:.4f} | {pval:.4f} | {is_sig}|\")\n",
    "        \n",
    "        if is_sig:\n",
    "            significant_cases.append({\n",
    "                'type': 'Global',\n",
    "                'sdg': f\"{sdg_name} ({sdg_id})\",\n",
    "                'target': label,\n",
    "                'quantile': 'All',\n",
    "                'p_value': pval,\n",
    "                'mean_diff': mean_exist - mean_non_exist\n",
    "            })\n",
    "            \n",
    "    print(\"\\n\")\n",
    "\n",
    "    # Quantile Analysis Helper\n",
    "    def run_quantile_analysis(revenue_col, target_col, target_label, quantile_label):\n",
    "        print(f\"[{quantile_label}, {target_label} mean comparison]\")\n",
    "        print(\"|quantile group| N(exist) | N(non-exist) | existence mean | non existence mean | p-value | is significant|\")\n",
    "        \n",
    "        temp_df = analysis_df[analysis_df[revenue_col] > 0].copy()\n",
    "        \n",
    "        try:\n",
    "            temp_df['quantile'] = pd.qcut(temp_df[revenue_col], 10, labels=False)\n",
    "        except ValueError:\n",
    "             temp_df['quantile'] = pd.qcut(temp_df[revenue_col].rank(method='first'), 10, labels=False)\n",
    "             \n",
    "        for q in range(10):\n",
    "            q_data = temp_df[temp_df['quantile'] == q]\n",
    "            \n",
    "            has_sdg_q = q_data['entity_id'].isin(entities_with_sdg)\n",
    "            g_exist = q_data[has_sdg_q][target_col].dropna()\n",
    "            g_non_exist = q_data[~has_sdg_q][target_col].dropna()\n",
    "            \n",
    "            n_exist = len(g_exist)\n",
    "            n_non_exist = len(g_non_exist)\n",
    "            \n",
    "            if n_exist > 1 and n_non_exist > 1:\n",
    "                stat, pval = stats.ttest_ind(g_exist, g_non_exist, equal_var=False)\n",
    "                m_exist = g_exist.mean()\n",
    "                m_non_exist = g_non_exist.mean()\n",
    "                is_sig = pval < 0.05\n",
    "            else:\n",
    "                m_exist = np.nan\n",
    "                m_non_exist = np.nan\n",
    "                pval = np.nan\n",
    "                is_sig = False\n",
    "            \n",
    "            print(f\"|{q}| {n_exist} | {n_non_exist} | {m_exist:.4f} | {m_non_exist:.4f} | {pval:.4f} | {is_sig}|\")\n",
    "            \n",
    "            if is_sig:\n",
    "                significant_cases.append({\n",
    "                    'type': 'Quantile',\n",
    "                    'sdg': f\"{sdg_name} ({sdg_id})\",\n",
    "                    'target': target_label,\n",
    "                    'quantile': f\"{quantile_label} (Group {q})\",\n",
    "                    'p_value': pval,\n",
    "                    'mean_diff': m_exist - m_non_exist\n",
    "                })\n",
    "        print(\"\\n\")\n",
    "\n",
    "    run_quantile_analysis('scope_1_revenue', 'target_scope_1', 'target scope 1', 'scope 1 revenue quantile')\n",
    "    run_quantile_analysis('scope_1_revenue', 'log_target_scope_1', 'target scope 1 log', 'scope 1 revenue quantile')\n",
    "    run_quantile_analysis('scope_2_revenue', 'target_scope_2', 'target scope 2', 'scope 2 revenue quantile')\n",
    "    run_quantile_analysis('scope_2_revenue', 'log_target_scope_2', 'target scope 2 log', 'scope 2 revenue quantile')\n",
    "    \n",
    "    print(\"-\" * 50 + \"\\n\")\n",
    "\n",
    "print(\"=== Summary of Significant Cases ===\")\n",
    "if significant_cases:\n",
    "    sig_df = pd.DataFrame(significant_cases)\n",
    "    # Sort by p-value for better readability\n",
    "    sig_df = sig_df.sort_values('p_value')\n",
    "    print(sig_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No significant cases found.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6802dfb1",
   "metadata": {},
   "source": [
    "# 7. Experiments\n",
    "\n",
    "To validate whether the insights from the earlier data analysis actually improve prediction of Scope 1 and Scope 2 emissions, several experiments were conducted. \n",
    "Each experiment combines a preprocessing strategy with a specific model type. The goal is to compare both datasets and modeling approaches in a controlled setting.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "Two dataset configurations are compared.\n",
    "\n",
    "### 1. Baseline Dataset\n",
    "This follows the preprocessing pipeline originally provided by the Codeathon organizers. It includes:\n",
    "\n",
    "- One-hot encoding of `region`  \n",
    "- Adding sector-level revenue share using the revenue distribution table  \n",
    "- Adding environmental activity score adjustments  \n",
    "- Adding binary features representing Sustainable Development Goal (SDG) activities  \n",
    "- Applying a variance threshold of 0.05 for feature selection  \n",
    "\n",
    "### 2. Proposed Dataset\n",
    "This dataset uses features identified through exploratory analysis and domain reasoning. The feature set includes:\n",
    "\n",
    "- `log_total_revenue`\n",
    "- `log_scope1_revenue`\n",
    "- `log_scope2_revenue`\n",
    "- `scope1_revenue_present` (0 or 1)\n",
    "- `scope2_revenue_present` (0 or 1)\n",
    "- `env_adjusted_score`\n",
    "- `social_score`\n",
    "- `governance_score`\n",
    "- `country_code`  \n",
    "  - Used directly as a categorical variable for tree-based models  \n",
    "  - One-hot encoded for linear models  \n",
    "\n",
    "The target variables (`target_scope_1`, `target_scope_2`) are log-transformed during training to stabilize variance. Predictions are converted back to raw scale for final evaluation.\n",
    "\n",
    "\n",
    "## Models\n",
    "\n",
    "Three different models are used to evaluate linear trends, robustness to outliers, and nonlinear relationships.\n",
    "\n",
    "### 1. Linear Regression\n",
    "This is the baseline model defined by the organizers. It optimizes MSE and serves as a reference point. Linear regression is sensitive to outliers and cannot capture nonlinearity.\n",
    "\n",
    "### 2. Median Regression\n",
    "Our primary metric is MAE, and secondary metrics are log MAE and log RMSE. Since all emphasize central tendency and robustness, median regression is appropriate. The regularization constant (alpha) is set to 0 to avoid biasing the solution.\n",
    "\n",
    "### 3. CatBoost\n",
    "Given the skewed numerical features and categorical variables such as `country_code`, a tree-based model is well suited. CatBoost handles categorical encoding internally and captures nonlinear effects efficiently, making it a strong candidate for small datasets.\n",
    "\n",
    "\n",
    "## Experiment Settings\n",
    "\n",
    "We evaluate four combinations of dataset and model:\n",
    "\n",
    "1. **Baseline Dataset + Linear Regression**  \n",
    "   This recreates the original baseline setup and serves as a comparison anchor.\n",
    "\n",
    "2. **Baseline Dataset + Median Regression**  \n",
    "   This tests whether replacing the baseline model with a MAE-oriented model improves performance under the same feature configuration.\n",
    "\n",
    "3. **Proposed Dataset + Median Regression**  \n",
    "   This evaluates whether the newly engineered features provide meaningful gains over the baseline features.\n",
    "\n",
    "4. **Proposed Dataset + CatBoost**  \n",
    "   This leverages a tree-based method to capture nonlinear patterns and directly handle categorical variables, aiming for the best overall performance.\n",
    "\n",
    "\n",
    "## Results\n",
    "\n",
    "The following table summarizes the cross-validated performance of all four experiment settings.  \n",
    "Metrics reported are MAE, log-MAE, and log-RMSE for both Scope 1 and Scope 2 targets.\n",
    "\n",
    "### Final Results Table\n",
    "\n",
    "| Model                                      | Scope 1 MAE | Scope 1 Log MAE | Scope 1 Log RMSE | Scope 2 MAE | Scope 2 Log MAE | Scope 2 Log RMSE |\n",
    "|--------------------------------------------|-------------|-----------------|------------------|-------------|-----------------|------------------|\n",
    "| LinearRegression + Baseline                | 64409.1131  | 2.5848          | 3.5073           | 71708.9193  | 3.1736          | 4.3514           |\n",
    "| MedianRegression + Baseline                | 51830.9031  | 2.0765          | 2.8194           | 52615.5835  | 2.1804          | 3.2238           |\n",
    "| MedianRegression + NewFeatures + LogTarget | 51025.5594  | 1.5479          | 1.9553           | 55099.4216  | 1.8250          | **2.4967**       |\n",
    "| CatBoost + NewFeatures + LogTarget         | **50094.6028** | **1.4946**     | **1.8922**       | **52235.8818** | **1.8224**     | 2.5350       |\n",
    "\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "### 1. Baseline vs Median Regression  \n",
    "- Median regression significantly outperforms linear regression across all metrics.  \n",
    "- This confirms that MAE-oriented modeling and robustness to outliers are important for this task.\n",
    "\n",
    "### 2. Baseline Features vs Proposed Features  \n",
    "- Replacing the baseline features with the proposed analytical features further improves performance.  \n",
    "- Log-transforming the targets stabilizes heavy skewness and reduces both log-MAE and log-RMSE substantially.\n",
    "\n",
    "### 3. CatBoost Performance  \n",
    "- CatBoost delivers the best results for Scope 1 and competitive results for Scope 2.  \n",
    "- Its ability to handle nonlinearity and categorical variables directly gives it an advantage.  \n",
    "- Overall, **CatBoost + Proposed Features + Log Target** is the top-performing setup.\n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Feature engineering** based on domain insights leads to consistent improvements.  \n",
    "- **Log-target modeling** is highly effective for emission data with heavy-tailed distributions.  \n",
    "- **CatBoost** is the strongest model overall, indicating that nonlinear relationships and categorical structure play an important role in predicting emissions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfeb89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from src.preprocessor import (\n",
    "    CodeathonBaselinePreprocessor,\n",
    "    ProposedFeaturePreprocessor\n",
    ")\n",
    "from src.models import (\n",
    "    LinearRegressionModel,\n",
    "    MedianRegressionModel,\n",
    "    LogTargetCatBoostModel,\n",
    "    LogTargetMedianRegressionModel\n",
    ")\n",
    "from src.trainer import Trainer\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Load Data\n",
    "# -----------------------------------------------------\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "y_scope1 = train_df['target_scope_1']\n",
    "y_scope2 = train_df['target_scope_2']\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Define Experiment Configurations\n",
    "# -----------------------------------------------------\n",
    "experiments = [\n",
    "    {\n",
    "        \"name\": \"LinearRegression + Baseline\",\n",
    "        \"preprocessor\": CodeathonBaselinePreprocessor(),\n",
    "        \"model_class\": LinearRegressionModel,\n",
    "        \"model_params\": {},\n",
    "        \"trainer_params\": {\"n_splits\": 5, \"random_state\": 42},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MedianRegression + Baseline\",\n",
    "        \"preprocessor\": CodeathonBaselinePreprocessor(),\n",
    "        \"model_class\": MedianRegressionModel,\n",
    "        \"model_params\": {},\n",
    "        \"trainer_params\": {\"n_splits\": 5, \"random_state\": 42},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MedianRegression + NewFeatures + LogTarget\",\n",
    "        \"preprocessor\": ProposedFeaturePreprocessor(),\n",
    "        \"model_class\": LogTargetMedianRegressionModel,\n",
    "        \"model_params\": {},\n",
    "        \"trainer_params\": {\"n_splits\": 5, \"random_state\": 42},\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"CatBoost + NewFeatures + LogTarget\",\n",
    "        \"preprocessor\": ProposedFeaturePreprocessor(tree=True),\n",
    "        \"model_class\": LogTargetCatBoostModel,\n",
    "        \"model_params\": {\n",
    "            'iterations': 100,\n",
    "            'depth': 6,\n",
    "            'learning_rate': 0.1,\n",
    "            'random_state': 42\n",
    "        },\n",
    "        \"trainer_params\": {\"n_splits\": 5, \"random_state\": 42},\n",
    "    },\n",
    "]\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Run Experiments\n",
    "# -----------------------------------------------------\n",
    "results = []\n",
    "\n",
    "for exp in experiments:\n",
    "    print(f\"\\n=== Running {exp['name']} ===\")\n",
    "\n",
    "    # Preprocess\n",
    "    X = exp[\"preprocessor\"].fit_transform(train_df)\n",
    "\n",
    "    # Trainer\n",
    "    trainer = Trainer(\n",
    "        model_class=exp[\"model_class\"],\n",
    "        model_params=exp[\"model_params\"],\n",
    "        **exp[\"trainer_params\"]\n",
    "    )\n",
    "\n",
    "    # Train scope 1\n",
    "    m1 = trainer.train(X, y_scope1)\n",
    "    # Train scope 2\n",
    "    m2 = trainer.train(X, y_scope2)\n",
    "\n",
    "    # Save results into table row\n",
    "    results.append({\n",
    "        \"model\": exp[\"name\"],\n",
    "        \"scope1_mae\": m1[\"mean_mae\"],\n",
    "        \"scope1_log_mae\": m1[\"mean_log_mae\"],\n",
    "        \"scope1_log_rmse\": m1[\"mean_log_rmse\"],\n",
    "        \"scope2_mae\": m2[\"mean_mae\"],\n",
    "        \"scope2_log_mae\": m2[\"mean_log_mae\"],\n",
    "        \"scope2_log_rmse\": m2[\"mean_log_rmse\"],\n",
    "    })\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Results DataFrame\n",
    "# -----------------------------------------------------\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n\\n==================== Final Results Table ====================\")\n",
    "print(\n",
    "    tabulate(\n",
    "        results_df,\n",
    "        headers=\"keys\",\n",
    "        tablefmt=\"github\",\n",
    "        floatfmt=\".4f\",\n",
    "        showindex=False\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d791d1d",
   "metadata": {},
   "source": [
    "# 8. CatBoost Hyperparameter Optimization\n",
    "\n",
    "This section runs a lightweight hyperparameter search to improve the CatBoost model using the proposed feature set and log-transformed targets.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- The proposed feature preprocessor is applied once (`tree=True`) and reused.\n",
    "- A simple **random search** with 20 trials is performed.\n",
    "- Each trial trains two CatBoost models (Scope 1, Scope 2) using 5-fold CV.\n",
    "- Optimization target is the average MAE across both scopes.\n",
    "\n",
    "## Search Space\n",
    "\n",
    "- `iterations`: 200–1000  \n",
    "- `depth`: 4–8  \n",
    "- `learning_rate`: 0.03–0.1  \n",
    "- `l2_leaf_reg`: 1–10  \n",
    "- `random_state`: 42  \n",
    "\n",
    "## Process\n",
    "\n",
    "For each trial:\n",
    "\n",
    "1. Sample hyperparameters  \n",
    "2. Train CatBoost (log-target mode)  \n",
    "3. Compute MAE for Scope 1 and Scope 2  \n",
    "4. Calculate `(MAE1 + MAE2) / 2`  \n",
    "5. Record the result  \n",
    "\n",
    "The best configuration is chosen by sorting on the combined MAE.\n",
    "\n",
    "## Final Step\n",
    "\n",
    "- Retrain CatBoost with the best hyperparameters  \n",
    "- Add results as a new row:  \n",
    "  **\"CatBoost + NewFeatures + LogTarget (Optimized)\"**  \n",
    "- Append to the previous experiment table\n",
    "\n",
    "\n",
    "# Results (Including Optimized CatBoost)\n",
    "\n",
    "### Final Results Table (With Optimization)\n",
    "\n",
    "| Model                                          | Scope 1 MAE | Scope 1 Log MAE | Scope 1 Log RMSE | Scope 2 MAE | Scope 2 Log MAE | Scope 2 Log RMSE |\n",
    "|------------------------------------------------|-------------|-----------------|------------------|-------------|-----------------|------------------|\n",
    "| LinearRegression + Baseline                    | 64409.1131  | 2.5848          | 3.5073           | 71708.9193  | 3.1736          | 4.3514           |\n",
    "| MedianRegression + Baseline                    | 51830.9031  | 2.0765          | 2.8194           | 52615.5835  | 2.1804          | 3.2238           |\n",
    "| MedianRegression + NewFeatures + LogTarget     | 51025.5594  | 1.5479          | 1.9553           | 55099.4216  | 1.8250          | 2.4967           |\n",
    "| CatBoost + NewFeatures + LogTarget             | 50094.6028  | **1.4946**      | 1.8922           | **52235.8818** | 1.8224        | 2.5350           |\n",
    "| **CatBoost + NewFeatures + LogTarget (Optimized)** | **49595.7693** | 1.4977      | **1.8918**       | 53014.5223  | **1.8212**      | **2.5132**       |\n",
    "\n",
    "### Best Values by Metric\n",
    "\n",
    "- **Scope 1 MAE:** Optimized CatBoost  \n",
    "- **Scope 1 Log MAE:** CatBoost (non-optimized)  \n",
    "- **Scope 1 Log RMSE:** Optimized CatBoost  \n",
    "- **Scope 2 MAE:** CatBoost (non-optimized)  \n",
    "- **Scope 2 Log MAE:** Optimized CatBoost  \n",
    "- **Scope 2 Log RMSE:** Optimized CatBoost  \n",
    "\n",
    "\n",
    "## Summary\n",
    "\n",
    "- CatBoost consistently outperforms all linear and median regression baselines.\n",
    "- Hyperparameter optimization **only provides minor improvement**, mainly in Scope 1 metrics.\n",
    "- The non-optimized CatBoost model already performs extremely well across all metrics.\n",
    "- Given the marginal gains and small dataset size, **extensive hyperparameter tuning is unnecessary** for this task.\n",
    "\n",
    "In conclusion, **CatBoost + Proposed Features + LogTarget** is sufficiently strong even without hyperparameter optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef15f335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from src.preprocessor import ProposedFeaturePreprocessor\n",
    "from src.models import LogTargetCatBoostModel\n",
    "from src.trainer import Trainer\n",
    "from tabulate import tabulate\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Settings\n",
    "# -----------------------------------------------------\n",
    "N_TRIALS = 20\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "random.seed(RANDOM_STATE)\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "y_scope1 = train_df[\"target_scope_1\"]\n",
    "y_scope2 = train_df[\"target_scope_2\"]\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Preprocess once (tree mode)\n",
    "# -----------------------------------------------------\n",
    "preprocessor = ProposedFeaturePreprocessor(tree=True)\n",
    "X = preprocessor.fit_transform(train_df)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Hyperparameter Search Space\n",
    "# -----------------------------------------------------\n",
    "def sample_params():\n",
    "    return {\n",
    "        \"iterations\": random.choice([200, 300, 500, 800, 1000]),\n",
    "        \"depth\": random.choice([4, 5, 6, 7, 8]),\n",
    "        \"learning_rate\": random.choice([0.03, 0.05, 0.07, 0.1]),\n",
    "        \"l2_leaf_reg\": random.choice([1, 3, 5, 7, 10]),\n",
    "        \"random_state\": RANDOM_STATE\n",
    "    }\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Optimization Loop\n",
    "# -----------------------------------------------------\n",
    "trial_results = []\n",
    "\n",
    "print(\"\\n=== CatBoost Hyperparameter Optimization ===\")\n",
    "\n",
    "for trial in range(1, N_TRIALS + 1):\n",
    "    params = sample_params()\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model_class=LogTargetCatBoostModel,\n",
    "        model_params=params,\n",
    "        n_splits=5,\n",
    "        random_state=RANDOM_STATE\n",
    "    )\n",
    "\n",
    "    m1 = trainer.train(X, y_scope1)\n",
    "    m2 = trainer.train(X, y_scope2)\n",
    "\n",
    "    combined_score = (m1[\"mean_mae\"] + m2[\"mean_mae\"]) / 2\n",
    "\n",
    "    trial_results.append({\n",
    "        \"trial\": trial,\n",
    "        **params,\n",
    "        \"scope1_mae\": m1[\"mean_mae\"],\n",
    "        \"scope2_mae\": m2[\"mean_mae\"],\n",
    "        \"combined\": combined_score\n",
    "    })\n",
    "\n",
    "    print(f\"Trial {trial}/{N_TRIALS}: combined MAE = {combined_score:.3f} | params = {params}\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Results data frame\n",
    "# -----------------------------------------------------\n",
    "hpo_results_df = pd.DataFrame(trial_results).sort_values(\"combined\")\n",
    "\n",
    "print(\"\\n\\n==================== Optimization Results ====================\")\n",
    "print(\n",
    "    tabulate(\n",
    "        hpo_results_df,\n",
    "        headers=\"keys\",\n",
    "        tablefmt=\"github\",\n",
    "        floatfmt=\".4f\",\n",
    "        showindex=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Extract best params\n",
    "# -----------------------------------------------------\n",
    "best_row = hpo_results_df.iloc[0].to_dict()\n",
    "best_params = {\n",
    "    key: best_row[key]\n",
    "    for key in [\"iterations\", \"depth\", \"learning_rate\", \"l2_leaf_reg\", \"random_state\"]\n",
    "}\n",
    "\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(best_params)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Retrain CatBoost using best params\n",
    "# =====================================================\n",
    "trainer = Trainer(\n",
    "    model_class=LogTargetCatBoostModel,\n",
    "    model_params=best_params,\n",
    "    n_splits=5,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "m1_best = trainer.train(X, y_scope1)\n",
    "m2_best = trainer.train(X, y_scope2)\n",
    "\n",
    "# Row to append\n",
    "optimized_row = {\n",
    "    \"model\": \"CatBoost + NewFeatures + LogTarget (Optimized)\",\n",
    "    \"scope1_mae\": m1_best[\"mean_mae\"],\n",
    "    \"scope1_log_mae\": m1_best[\"mean_log_mae\"],\n",
    "    \"scope1_log_rmse\": m1_best[\"mean_log_rmse\"],\n",
    "    \"scope2_mae\": m2_best[\"mean_mae\"],\n",
    "    \"scope2_log_mae\": m2_best[\"mean_log_mae\"],\n",
    "    \"scope2_log_rmse\": m2_best[\"mean_log_rmse\"],\n",
    "}\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Append to previous cell's results_df\n",
    "# =====================================================\n",
    "try:\n",
    "    final_results_df = pd.concat(\n",
    "        [results_df, pd.DataFrame([optimized_row])],\n",
    "        ignore_index=True\n",
    "    )\n",
    "\n",
    "    print(\"\\n\\n==================== Final Results Table (With Optimization) ====================\")\n",
    "    print(\n",
    "        tabulate(\n",
    "            final_results_df,\n",
    "            headers=\"keys\",\n",
    "            tablefmt=\"github\",\n",
    "            floatfmt=\".4f\",\n",
    "            showindex=False\n",
    "        )\n",
    "    )\n",
    "\n",
    "except NameError:\n",
    "    print(\"\\n[WARNING] previous cell's results_df not found.\")\n",
    "    print(\"Optimized row:\")\n",
    "    print(optimized_row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a641656",
   "metadata": {},
   "source": [
    "# 9. Final Prediction (submission.csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6d2bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessor import ProposedFeaturePreprocessor\n",
    "from src.models import LogTargetCatBoostModel\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"\\n--- Training Final Models and Generating Submission ---\")\n",
    "\n",
    "# Load data\n",
    "train_df = pd.read_csv('data/train.csv')\n",
    "test_df = pd.read_csv('data/test.csv')\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = ProposedFeaturePreprocessor(tree=True)\n",
    "\n",
    "# Fit on train and transform\n",
    "X = preprocessor.fit_transform(train_df)\n",
    "X_test = preprocessor.transform(test_df)\n",
    "\n",
    "# Extract targets\n",
    "y_scope1 = train_df['target_scope_1']\n",
    "y_scope2 = train_df['target_scope_2']\n",
    "\n",
    "# Model params (from final.ipynb:L49-L60)\n",
    "model_params = {\n",
    "    'iterations': 100,\n",
    "    'depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Train Scope 1 on full data\n",
    "print(\"Training Scope 1...\")\n",
    "model_s1 = LogTargetCatBoostModel(**model_params)\n",
    "model_s1.fit(X, y_scope1)\n",
    "s1_predictions = model_s1.predict(X_test)\n",
    "\n",
    "# Train Scope 2 on full data\n",
    "print(\"Training Scope 2...\")\n",
    "model_s2 = LogTargetCatBoostModel(**model_params)\n",
    "model_s2.fit(X, y_scope2)\n",
    "s2_predictions = model_s2.predict(X_test)\n",
    "\n",
    "# Create submission DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'entity_id': test_df['entity_id'],\n",
    "    's1_predictions': s1_predictions,\n",
    "    's2_predictions': s2_predictions\n",
    "})\n",
    "\n",
    "# Save submission\n",
    "submission.to_csv(\"submission.csv\", index=False)\n",
    "print(\"Submission saved to submission.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e60d9b",
   "metadata": {},
   "source": [
    "# 10. Conclusion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
